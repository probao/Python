###################################
###numpy基础：数组和矢量的计算#####
###################################


创建ndarray
>>> import numpy as np
>>> data1 = [6, 7.5, 8, 0, 1]
>>> arr1 = np.array(data1)	# 使用array函数，它接受一切序列型的对象，产生一个含有传入数据的numpy数组
>>> arr1
array([ 6. ,  7.5,  8. ,  0. ,  1. ])


嵌套序列
>>> data2 = [[1,2,3,4], [5, 6, 7, 8]]
>>> arr2 = np.array(data2)
>>> arr2
array([[1, 2, 3, 4],
       [5, 6, 7, 8]])
>>> arr2.ndim	# 数组的维数
2
>>> arr2.shape	# 2行4列的数组
(2, 4)

>>> arr1.dtype	# 数据类型保存在一个特殊的dtype对象中
dtype('float64')
>>> arr2.dtype
dtype('int32')


其它函数新建数组
>>> np.zeros(10)	# 生成全是0的数组
array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])
>>> np.zeros((3, 6))
array([[ 0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.]])
>>> np.ones((3, 6))	# 生成全是1的数组
array([[ 1.,  1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.,  1.]])
>>> np.empty((2, 3, 2))		# 生成的不是0， 是一些未被处理的垃圾值
array([[[  1.03510883e-295,   6.38742701e-314],
        [  1.55420853e-285,   1.27319747e-313],
        [  1.27319747e-313,   1.27319747e-313]],

       [[  2.37332014e-286,   1.91205622e-313],
        [  8.13462439e-296,   2.75859453e-313],
        [  2.48324480e-286,   3.12237178e-120]]])

>>> np.arange(15)		# arange是python内置函数range的数组版
array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])

>>> np.eye(3)			# 对角线是1的数组
array([[ 1.,  0.,  0.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.]])


ndarray的数据类型
>>> arr1 = np.array([1, 2, 3], dtype=np.float64)
>>> arr2 = np.array([1, 2, 3], dtype=np.int32)
>>> arr1.dtype
dtype('float64')
>>> arr2.dtype
dtype('int32')



数组和标量之间的运算
>>> arr = np.array([[1., 2., 3.], [4., 5., 6.]])
>>> arr
array([[ 1.,  2.,  3.],
       [ 4.,  5.,  6.]])
>>> arr*arr
array([[  1.,   4.,   9.],	#大小相等的数组的任何运算都会将运算运用到元素级
       [ 16.,  25.,  36.]])
>>> 1/arr
array([[ 1.        ,  0.5       ,  0.33333333],
       [ 0.25      ,  0.2       ,  0.16666667]])
>>> arr**0.5
array([[ 1.        ,  1.41421356,  1.73205081],
       [ 2.        ,  2.23606798,  2.44948974]])

>>> a = np.array([[1,2],[1,2]])	#矩阵的运算需要dot函数
>>> np.dot(a,a)
array([[3, 6],
       [3, 6]])


基本的索引和切片
>>> import numpy as np
>>> arr = np.arange(10)
>>> arr
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>>
>>>
>>> arr[5]
5
>>>
>>> arr[5:8]
array([5, 6, 7])
>>> arr[5:8] = 12
>>> arr
array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9]) # 当一个标量值赋值给一个切片时，该值会自动传播到整个选取


>>> arr_slice = arr[5:8]
>>> arr_slice[1] = 12345
>>> arr
array([    0,     1,     2,     3,     4,    12, 12345,    12,     8,     9])
>>> arr_slice[:] = 64
>>> arr
array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])


>>> arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])	# 多维数组索引
>>> arr2d[2]
array([7, 8, 9])
>>> arr2d[0][2]
3
>>> arr2d[0, 2]
3


>>> arr3d = np.array([[[1, 2, 3], [4, 5, 6]],[[7, 8, 9], [10, 11, 12]]])
>>> arr3d	# 括号外面的“维度”是一维，二维，。。。而括号里面的应该理解为“轴”--类比三维数组
array([[[ 1,  2,  3],
        [ 4,  5,  6]],

       [[ 7,  8,  9],
        [10, 11, 12]]])
>>> arr3d [0]
array([[1, 2, 3],
       [4, 5, 6]])



>>> old_values = arr3d[0].copy()
>>>
>>> arr3d[0] = 42
>>> arr3d
array([[[42, 42, 42],
        [42, 42, 42]],

       [[ 7,  8,  9],
        [10, 11, 12]]])
>>> arr3d[0] = old_values	# 标量和数组都可以被赋值给arr3d[0]
>>> arr3d
array([[[ 1,  2,  3],
        [ 4,  5,  6]],

       [[ 7,  8,  9],
        [10, 11, 12]]])

>>> arr3d[1, 0]
array([7, 8, 9])

切片索引

>>> arr
array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])
>>> arr[1:6]
array([ 1,  2,  3,  4, 64])


>>> arr2d
array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])
>>> arr2d[:2]	#[:2]意思是--第0项和第1项
array([[1, 2, 3],
       [4, 5, 6]])
>>> arr2d[:2, 1:]	# [1:] 第一项到最后
array([[2, 3],
       [5, 6]])


>>> arr2d[1, :2]
array([4, 5])
>>> arr2d[2, :1]
array([7])
>>> arr2d[:, :1]	
array([[1],
       [4],
       [7]])
>>> arr2d[:2, 1:]
array([[2, 3],
       [5, 6]])
>>> arr2d[:2, 1:] = 0	#对切片表达式的赋值操作也会被扩赛到整个选区
>>> arr2d
array([[1, 0, 0],
       [4, 0, 0],
       [7, 8, 9]])


布尔型索引

>>> data = np.random.randn(7, 4)
>>> data
array([[-0.31042535, -2.17572506,  0.56252532,  0.99534791],
       [ 0.63397486, -0.13189191,  0.29471484,  0.50351773],
       [ 0.51495017,  0.04994144, -0.1762269 , -0.19449053],
       [-0.07743183, -0.42964157, -1.39686796, -1.11814711],
       [-1.13909174, -0.65366514, -0.2169853 ,  0.61072596],
       [ 1.41913738, -1.2421399 ,  0.45052187, -0.17214591],
       [-0.14628746,  0.25535978, -1.04384285,  0.45418343]])
>>> names
array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'],
      dtype='|S4')
>>> names == 'Bob'
array([ True, False, False,  True, False, False, False], dtype=bool)
>>> data[names ==  'Bob']	# 布尔型数组可用于数组索引，布尔型数组的长度必须跟被索引的轴长度一致
array([[-0.31042535, -2.17572506,  0.56252532,  0.99534791],
       [-0.07743183, -0.42964157, -1.39686796, -1.11814711]])


>>> data[names ==  'Bob', 2:]	# 布尔型数组跟切片混合使用
array([[ 0.56252532,  0.99534791],
       [-1.39686796, -1.11814711]])

>>> data[names ==  'Bob', 3]
array([ 0.99534791, -1.11814711])

>>> names != 'Bob'	# 选择除“Bob”以外的其他值可以使用不等于符号(!=),也可以通过负号
array([False,  True,  True, False,  True,  True,  True], dtype=bool)
>>> data[-(names ==  'Bob')]
array([[ 0.63397486, -0.13189191,  0.29471484,  0.50351773],
       [ 0.51495017,  0.04994144, -0.1762269 , -0.19449053],
       [-1.13909174, -0.65366514, -0.2169853 ,  0.61072596],
       [ 1.41913738, -1.2421399 ,  0.45052187, -0.17214591],
       [-0.14628746,  0.25535978, -1.04384285,  0.45418343]])


>>> mask = (names == 'Bob')|(names == 'Will')
>>> mask
array([ True, False,  True,  True,  True, False, False], dtype=bool)
>>> mask1 = (names == 'Bob') & (names == 'Will')
>>> mask1
array([False, False, False, False, False, False, False], dtype=bool)
>>> data[mask]
array([[-0.31042535, -2.17572506,  0.56252532,  0.99534791],
       [ 0.51495017,  0.04994144, -0.1762269 , -0.19449053],
       [-0.07743183, -0.42964157, -1.39686796, -1.11814711],
       [-1.13909174, -0.65366514, -0.2169853 ,  0.61072596]])


>>> data
array([[-0.31042535, -2.17572506,  0.56252532,  0.99534791],
       [ 0.63397486, -0.13189191,  0.29471484,  0.50351773],
       [ 0.51495017,  0.04994144, -0.1762269 , -0.19449053],
       [-0.07743183, -0.42964157, -1.39686796, -1.11814711],
       [-1.13909174, -0.65366514, -0.2169853 ,  0.61072596],
       [ 1.41913738, -1.2421399 ,  0.45052187, -0.17214591],
       [-0.14628746,  0.25535978, -1.04384285,  0.45418343]])
>>>
>>> data[data < 0] = 0	#将data中的所有负值都设置为0
>>> data
array([[ 0.        ,  0.        ,  0.56252532,  0.99534791],
       [ 0.63397486,  0.        ,  0.29471484,  0.50351773],
       [ 0.51495017,  0.04994144,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ,  0.61072596],
       [ 1.41913738,  0.        ,  0.45052187,  0.        ],
       [ 0.        ,  0.25535978,  0.        ,  0.45418343]])

>>> data[names != 'Joe'] = 7	#
>>> data
array([[ 7.        ,  7.        ,  7.        ,  7.        ],
       [ 0.63397486,  0.        ,  0.29471484,  0.50351773],
       [ 7.        ,  7.        ,  7.        ,  7.        ],
       [ 7.        ,  7.        ,  7.        ,  7.        ],
       [ 7.        ,  7.        ,  7.        ,  7.        ],
       [ 1.41913738,  0.        ,  0.45052187,  0.        ],
       [ 0.        ,  0.25535978,  0.        ,  0.45418343]])


花式索引
>>> arr = np.empty((8,4))
>>> for i in range(8):
...     arr[i] = i
...
>>> arr
array([[ 0.,  0.,  0.,  0.],
       [ 1.,  1.,  1.,  1.],
       [ 2.,  2.,  2.,  2.],
       [ 3.,  3.,  3.,  3.],
       [ 4.,  4.,  4.,  4.],
       [ 5.,  5.,  5.,  5.],
       [ 6.,  6.,  6.,  6.],
       [ 7.,  7.,  7.,  7.]])
>>> arr[[4, 3, 0, 6]]	# 第四行，第三行，第零行 和 第六行
array([[ 4.,  4.,  4.,  4.],
       [ 3.,  3.,  3.,  3.],
       [ 0.,  0.,  0.,  0.],
       [ 6.,  6.,  6.,  6.]])

>>> arr[[-3, -5, 0, -7]]	#使用负数索引将会从末尾开始选取行
array([[ 5.,  5.,  5.,  5.],
       [ 3.,  3.,  3.,  3.],
       [ 0.,  0.,  0.,  0.],
       [ 1.,  1.,  1.,  1.]])


>>> arr = np.arange(32).reshape((8, 4)) # 一次传入多个索引数组，它返回的是一个一维数组，其中的元素对应各个索引元组
>>> arr
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15],
       [16, 17, 18, 19],
       [20, 21, 22, 23],
       [24, 25, 26, 27],
       [28, 29, 30, 31]])
>>> arr[[1, 5, 7, 2],[0, 3, 1, 2]]
array([ 4, 23, 29, 10])

>>> arr[[1, 5, 7, 2]][:,[0, 3, 1, 2]]	# 先取行，再取列
array([[ 4,  7,  5,  6],
       [20, 23, 21, 22],
       [28, 31, 29, 30],
       [ 8, 11,  9, 10]])



数组转置和轴对换
>>> arr = np.arange(15).reshape((3, 5))
>>> arr
array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14]])
>>> arr.T
array([[ 0,  5, 10],
       [ 1,  6, 11],
       [ 2,  7, 12],
       [ 3,  8, 13],
       [ 4,  9, 14]])



>>> arr = np.arange(16).reshape((2, 2, 4))
>>> arr
array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7]],

       [[ 8,  9, 10, 11],
        [12, 13, 14, 15]]])
>>> arr.transpose((1, 0, 2))
array([[[ 0,  1,  2,  3],
        [ 8,  9, 10, 11]],

       [[ 4,  5,  6,  7],
        [12, 13, 14, 15]]])
>>> arr.transpose((1, 2, 0))
array([[[ 0,  8],
        [ 1,  9],
        [ 2, 10],
        [ 3, 11]],

       [[ 4, 12],
        [ 5, 13],
        [ 6, 14],
        [ 7, 15]]])

我说你来试一下哈：
arr1.shape 应该是(2, 2, 4) 意为 2维，2*4矩阵

arr1.transpose(*args) 里面的参数，可以这么理解，他是调换arr1.shape的顺序，咱来给arr1.shape标一下角标哈，（2[0], 2[1], 4[2]）  [ ] 里是shape的索引，对吧， 
transpose((1, 0, 2)) 的意思是 按照这个顺序 重新设置shape 也就是 （2[1], 2[0], 4[2]）

 虽然看起来 变换前后的shape都是 2,2,4  ， 但是问题来了，transpose是转置
shape按照(1,0,2)的顺序重新设置了， array里的所有元素 也要按照这个规则重新组成新矩阵

 比如 8 在arr1中的索引是 (1, 0, 0)  那么按照刚才的变换规则，就是 (0, 1, 0) 看看跟你结果arr2的位置一样了吧，依此类推...



通用函数:快速的元素级数组函数
>>> import numpy as np
>>> arr = np.arange(10)
>>> np.sqrt(arr)
array([ 0.        ,  1.        ,  1.41421356,  1.73205081,  2.        ,
        2.23606798,  2.44948974,  2.64575131,  2.82842712,  3.        ])
>>> np.exp(arr)
array([  1.00000000e+00,   2.71828183e+00,   7.38905610e+00,
         2.00855369e+01,   5.45981500e+01,   1.48413159e+02,
         4.03428793e+02,   1.09663316e+03,   2.98095799e+03,
         8.10308393e+03])
sqrt和exp是一元（unary）ufunc，另外一些（如add或maximum）接受2个数组，也叫二元数组

>>> x = np.random.randn(8)
>>> y = np.random.randn(8)
>>> x
array([-1.09442604, -1.43680231, -0.28154207,  0.84074478, -0.18878751,
        0.50501874,  0.28835676,  1.85865484])
>>> y
array([ 0.97924522, -0.94704341,  0.04190887,  1.60123804,  0.12770678,
        0.04152567, -0.11783759, -0.75930132])
>>> np.maximum(x, y)
array([ 0.97924522, -0.94704341,  0.04190887,  1.60123804,  0.12770678,
        0.50501874,  0.28835676,  1.85865484])

>>> arr = np.random.randn(7)*5
>>> arr
array([ -9.17367478,   0.71396009, -10.1828497 ,   0.45724798,
        -1.05427677,   3.08732191,  -0.73990597])
>>> np.modf(arr)	# 返回两个数组 一个是小数部分，一个是整数部分
(array([-0.17367478,  0.71396009, -0.1828497 ,  0.45724798, -0.05427677,
        0.08732191, -0.73990597]), array([ -9.,   0., -10.,   0.,  -1.,   3.,  -0.]))



利用数组进行数据处理
>>> points = np.arange(-5, 5, 0.01)
>>> xs, ys = np.meshgrid(points, points)	#生成网格，两个二维数组
>>> ys
array([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],
       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],
       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],
       ...,
       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],
       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],
       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])
>>> xs
array([[-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],
       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],
       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],
       ...,
       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],
       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99],
       [-5.  , -4.99, -4.98, ...,  4.97,  4.98,  4.99]])
>>> import matplotlib.pyplot as plt
>>> z = np.sqrt(xs**2 + ys**2)
>>> z
array([[ 7.07106781,  7.06400028,  7.05693985, ...,  7.04988652,
         7.05693985,  7.06400028],
       [ 7.06400028,  7.05692568,  7.04985815, ...,  7.04279774,
         7.04985815,  7.05692568],
       [ 7.05693985,  7.04985815,  7.04278354, ...,  7.03571603,
         7.04278354,  7.04985815],
       ...,
       [ 7.04988652,  7.04279774,  7.03571603, ...,  7.0286414 ,
         7.03571603,  7.04279774],
       [ 7.05693985,  7.04985815,  7.04278354, ...,  7.03571603,
         7.04278354,  7.04985815],
       [ 7.06400028,  7.05692568,  7.04985815, ...,  7.04279774,
         7.04985815,  7.05692568]])
>>> plt.imshow(z, cmap=plt.cm.gray);plt.colorbar()
<matplotlib.image.AxesImage object at 0x062666B0>
<matplotlib.colorbar.Colorbar object at 0x062BE0D0>
>>> plt.show()


将条件逻辑表述为数组运算
假设我们要根据cond中的值选取xarr和yarr的值：当cond中的值为True时，选取xarr的值
否则从yarr中选取
>>> import numpy as np
>>> xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
>>> yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
>>> cond = np.array([True, False, True, True, False])
>>> result = np.where(cond, xarr, yarr)
>>> result
array([ 1.1,  2.2,  1.3,  1.4,  2.5])

np.where的第二个和第三个参数不必是数组，它们都是可以是标量值。在数据分析工作中
，where通常用于根据另一个数组而产生一个新的数组。


>>> arr = np.random.randn(4,4)
>>> arr
array([[-0.32285005,  0.25847681,  0.66953569,  1.7002404 ],
       [ 0.22827269,  1.02391622, -0.8919314 ,  0.38018606],
       [ 0.3845627 ,  0.84781941, -0.28423424,  1.73434853],
       [-0.76948816,  0.36355368,  1.56517939,  0.00546952]])
>>> np.where(arr > 0, 2, -2)	# 将数组中大于零的换成2，小于零的换成-2
array([[-2,  2,  2,  2],
       [ 2,  2, -2,  2],
       [ 2,  2, -2,  2],
       [-2,  2,  2,  2]])
>>> np.where(arr > 0, 2, arr)	# 只将数组中大于零的换成2
array([[-0.32285005,  2.        ,  2.        ,  2.        ],
       [ 2.        ,  2.        , -0.8919314 ,  2.        ],
       [ 2.        ,  2.        , -0.28423424,  2.        ],
       [-0.76948816,  2.        ,  2.        ,  2.        ]])



数学和统计方法
>>> arr = np.random.randn(5,4)
>>> arr
array([[-1.71504642,  0.22678685,  0.74513757,  1.70279446],
       [-0.34902157,  0.27176852, -1.52373216, -0.24647356],
       [-0.49901881, -0.11396496, -0.00902927, -0.30132351],
       [-0.44314036, -0.10644648, -1.3300278 , -3.00895452],
       [-0.26927576, -0.61164494, -1.39029488, -0.08084923]])
>>> arr.mean()		# 数组的平均数
-0.45258784121946649
>>> np.mean(arr)	# 数组的平均数
-0.45258784121946649
>>> arr.sum()
-9.0517568243893294	# 数组的和
>>> arr
array([[-1.71504642,  0.22678685,  0.74513757,  1.70279446],
       [-0.34902157,  0.27176852, -1.52373216, -0.24647356],
       [-0.49901881, -0.11396496, -0.00902927, -0.30132351],
       [-0.44314036, -0.10644648, -1.3300278 , -3.00895452],
       [-0.26927576, -0.61164494, -1.39029488, -0.08084923]])
>>> arr.mean(axis = 1)		# 每一行的平均数
array([ 0.23991811, -0.46186469, -0.23083414, -1.22214229, -0.5880162 ])
>>> arr.mean(axis = 0)		# 每一列的平均数
array([-0.65510058, -0.0667002 , -0.70158931, -0.38696127])


>>> arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
>>> arr.cumsum(0)	# 沿列的方向 每一行加下一行
array([[ 0,  1,  2],
       [ 3,  5,  7],
       [ 9, 12, 15]])
>>> arr.cumsum(1)	# 沿行的方向 每一列加下一列
array([[ 0,  1,  3],
       [ 3,  7, 12],
       [ 6, 13, 21]])
>>> arr.cumprod(1)
array([[  0,   0,   0],
       [  3,  12,  60],
       [  6,  42, 336]])
>>> arr.cumprod(0)
array([[ 0,  1,  2],
       [ 0,  4, 10],
       [ 0, 28, 80]])


用于布尔型数组的方法
>>> import numpy as np
>>> arr = np.random.randn(100)
>>> (arr > 0).sum()	# sum经常被用来对布尔型数组
58

>>> bools = np.array([False, False, True, False])
>>> bools.any()	# 用于测试有一个或多个True
True
>>> bools.all()	# 用于测试数组中是否全是True
False
这两个方法也能用于非布尔型数组，所有非0元素会被当做True


排序
>>> arr = np.random.randn(8)
>>> arr
array([-1.50790127, -1.22640436, -0.00729792, -0.42537421, -0.93833854,
       -0.6814844 ,  1.32235945, -0.09862056])
>>> arr.sort()
>>> arr
array([-1.50790127, -1.22640436, -0.93833854, -0.6814844 , -0.42537421,
       -0.09862056, -0.00729792,  1.32235945])


>>> arr = np.random.randn(3,3)
>>> arr
array([[-1.61589688, -0.65173118,  0.14666864],
       [ 0.87499686,  0.50801178, -0.06954368],
       [ 1.0262651 ,  1.08821325, -0.45640651]])
>>> arr.sort(1)		# 水平轴向sort
>>> arr
array([[-1.61589688, -0.65173118,  0.14666864],
       [-0.06954368,  0.50801178,  0.87499686],
       [-0.45640651,  1.0262651 ,  1.08821325]])


>>> arr = np.random.randn(3,3)
>>> arr
array([[ 1.42633942, -0.48664113,  0.02979959],
       [ 0.62360729,  0.57903973,  1.68476841],
       [ 0.94767599, -0.15832117,  0.97161712]])
>>> np.sort(arr)	# np.sort是已排序数组的副本
array([[-0.48664113,  0.02979959,  1.42633942],
       [ 0.57903973,  0.62360729,  1.68476841],
       [-0.15832117,  0.94767599,  0.97161712]])
>>> arr
array([[ 1.42633942, -0.48664113,  0.02979959],
       [ 0.62360729,  0.57903973,  1.68476841],
       [ 0.94767599, -0.15832117,  0.97161712]])

>>> large_arr = np.random.randn(1000)	# 计算数组分位数最简单的办法是对其进行排序，然后选取特定位置的值
>>> large_arr.sort()
>>> large_arr[int(0.05*len(large_arr))]
-1.5109867022543568


唯一化以及其他的集合逻辑
>>> import numpy as np
>>> names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])
>>> names
array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'],
      dtype='|S4')
>>> np.unique(names)	# 找出数组中的唯一值并返回已排序的结果
array(['Bob', 'Joe', 'Will'],
      dtype='|S4')

>>> ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])
>>> np.unique(ints)
array([1, 2, 3, 4])


>>> values = np.array([6, 0, 0, 3, 2, 5, 6])	# 另一个函数np.in1d用于测试一个数组中的
>>> np.in1d(values, [2, 3, 6])			# 值在另一个数组中的成员资格，返回一个布尔型数组
array([ True, False, False,  True,  True, False,  True], dtype=bool)


线性代数

>>> import numpy as np
>>> x = np.array([[1, 2, 3], [4, 5, 6]])
>>> y = np.array([[6, 23], [-1, 7], [8, 9]])
>>> x
array([[1, 2, 3],
       [4, 5, 6]])
>>> y
array([[ 6, 23],
       [-1,  7],
       [ 8,  9]])
>>> np.dot(x, y)
array([[ 28,  64],
       [ 67, 181]])


>>> z = np.ones(3)
>>> z
array([ 1.,  1.,  1.])
>>> x
array([[1, 2, 3],
       [4, 5, 6]])
>>> np.dot(x, z)
array([  6.,  15.])


>>> from numpy.linalg import inv, qr
>>> X = np.array([[2,3],[4,5]])
>>> X
array([[2, 3],
       [4, 5]])
>>> M = X.T	# 求X的转置矩阵
>>> M
array([[2, 4],	
       [3, 5]])
>>> X.T.dot(X)	# 先求X的转置矩阵然后与X点乘，XT*X
array([[20, 26],
       [26, 34]])

>>> X
array([[2, 3],
       [4, 5]])
>>> inv(X)	# inv求X的逆矩阵
array([[-2.5,  1.5],
       [ 2. , -1. ]])
>>> inv(X).dot(X)	# 互逆矩阵乘积为I
array([[ 1.,  0.],
       [ 0.,  1.]])


随机数生成
>>> nsteps = 1000
>>> draws = np.random.randint(0, 2, size=nsteps)	# 随机生成100
>>> steps = np.where(draws > 0, 1, -1)
>>> walk = steps.cumsum()
>>> walk.min()
-24
>>> walk.max()
19




###################################
############pandas入门#############
###################################


>>> from pandas import Series, DataFrame
>>> import pandas as pd
>>> obj = Series([4, 7, -5, 3])
>>> obj
0    4
1    7
2   -5
3    3
dtype: int64

>>> obj.values
array([ 4,  7, -5,  3])
>>> obj.index
RangeIndex(start=0, stop=4, step=1)
>>>
>>> obj2 = Series([4, 7, -5, 3], index = ['d', 'b', 'a', 'c'])
>>> obj2
d    4
b    7
a   -5
c    3
dtype: int64
>>> obj2['a']
-5
>>> obj2.index
Index([u'd', u'b', u'a', u'c'], dtype='object')

与普通NumPy数组相比，你可以通过索引的方式选取Series中的单个或一组值：
>>> from pandas import Series, DataFrame
>>> import pandas as pd
>>> obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
>>> obj2['a']
-5
>>> obj2['d'] = 10
>>> obj2
d    10
b     7
a    -5
c     3
dtype: int64
>>> obj2[['c', 'a', 'd']]
c     3
a    -5
d    10
dtype: int64


NumPy数组运算（如根据布尔型数组进行过滤、标量乘法、应用数学函数等）都会保留索引和
值之间的链接
>>> obj2
d    10
b     7
a    -5
c     3
dtype: int64
>>> obj2[obj2 > 0]
d    10
b     7
c     3
dtype: int64

>>> obj2*2
d    20
b    14
a   -10
c     6

>>> import numpy as np
>>> np.exp(obj2)
d    22026.465795
b     1096.633158
a        0.006738
c       20.085537


将Series看成是一个定长的有序字典，因为它是索引值到数据值的一个映射。
它可以用在许多原本需要字典参数的函数中
>>> 'b' in obj2
True
>>> 'e' in obj2
False

如果数据被放在一个Python字典中，也可以直接通过这个字典来创建Series：
>>> sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon' : 16000,
>>> obj3 = Series(sdata)
>>> obj3
Ohio      35000
Oregon    16000
Texas     71000
Utah       5000
dtype: int64

sdata中跟states索引相匹配的那3个值会被找出来并放到相应
的位置上，但由于"California"所对应的sdata值找不到，所以其结果为NaN
>>> states = ['California', 'Ohio', 'Oregon', 'Texas']
>>> obj4 = Series(sdata, index = states)
>>> obj4
California        NaN
Ohio          35000.0
Oregon        16000.0
Texas         71000.0
dtype: float64


pandas的isnull和notnull函数可用于检测缺失数据：
>>> pd.isnull(obj4)
California     True
Ohio          False
Oregon        False
Texas         False
dtype: bool
>>> pd.notnull(obj4)
California    False
Ohio           True
Oregon         True
Texas          True
dtype: bool


Series最重要的一个功能是：它在算术运算中会自动
对齐不同索引的数据
>>> obj3
Ohio      35000
Oregon    16000
Texas     71000
Utah       5000
dtype: int64
>>> obj4
California        NaN
Ohio          35000.0
Oregon        16000.0
Texas         71000.0
dtype: float64
>>> obj3 + obj4
California         NaN
Ohio           70000.0
Oregon         32000.0
Texas         142000.0
Utah               NaN
dtype: float64


Series对象本身及其索引都有一个name属性，该属性跟pandas其他的
功能非常密切：
>>> obj4.name = 'population'
>>> obj4.index.name = 'state'
>>> obj4
state
California        NaN
Ohio          35000.0
Oregon        16000.0
Texas         71000.0
Name: population, dtype: float64


Series的索引可以通过赋值的方式就地修改：
>>> obj = Series([4, 7, -5, 3])
>>> obj
0    4
1    7
2   -5
3    3
dtype: int64
>>> obj
0    4
1    7
2   -5
3    3
dtype: int64
>>> obj.index=['Bob', 'Steve', 'Jeff', 'Ryan']
>>> obj
Bob      4
Steve    7
Jeff    -5
Ryan     3
dtype: int64


DataFrame表格型数据结构
>>> data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
... 'year': [2000, 2001, 2002, 2001, 2002],
... 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}
>>> frame = DataFrame(data)
>>> frame
   pop   state  year
0  1.5    Ohio  2000
1  1.7    Ohio  2001
2  3.6    Ohio  2002
3  2.4  Nevada  2001
4  2.9  Nevada  2002


如果指定了列序列，则DataFrame的列就会按照指定顺序进行排序
>>> DataFrame(data, columns = ['year', 'state', 'pop'])
   year   state  pop
0  2000    Ohio  1.5
1  2001    Ohio  1.7
2  2002    Ohio  3.6
3  2001  Nevada  2.4
4  2002  Nevada  2.9

跟Series一样，如果传入的列在数据中找不到，就会产生NA值：
>>> frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'],
... index = ['one', 'two', 'three', 'four', 'five'])
>>> frame2
       year   state  pop debt
one    2000    Ohio  1.5  NaN
two    2001    Ohio  1.7  NaN
three  2002    Ohio  3.6  NaN
four   2001  Nevada  2.4  NaN
five   2002  Nevada  2.9  NaN



通过类似字典标记的方式或属性的方式，可以将DataFrame的列获取为一个Series：
>>> frame2['state']
one        Ohio
two        Ohio
three      Ohio
four     Nevada
five     Nevada
Name: state, dtype: object
>>> frame['year']
0    2000
1    2001
2    2002
3    2001
4    2002
Name: year, dtype: int64

行也可以通过位置或名称的方式进行获取，比如用索引字段ix
>>> frame2.ix['three']
year     2002
state    Ohio
pop       3.6
debt      NaN

>>> frame2
       year   state  pop debt
one       1    Ohio  1.5  NaN
two    2001    Ohio  1.7  NaN
three  2002    Ohio  3.6  NaN
four   2001  Nevada  2.4  NaN
five   2002  Nevada  2.9  NaN
>>> frame2['debt']['one']=2
>>> frame2
       year   state  pop debt
one       1    Ohio  1.5    2
two    2001    Ohio  1.7  NaN
three  2002    Ohio  3.6  NaN
four   2001  Nevada  2.4  NaN
five   2002  Nevada  2.9  NaN


列可以通过赋值的方式进行修改。
>>> frame2['debt'] = np.arange(5.)
>>> frame2
       year   state  pop  debt
one    2000    Ohio  1.5   0.0
two    2001    Ohio  1.7   1.0
three  2002    Ohio  3.6   2.0
four   2001  Nevada  2.4   3.0
five   2002  Nevada  2.9   4.0


将列表或数组赋值给某个列时，其长度必须跟DataFrame的长度相匹配。
>>> val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])
>>> frame2['debt'] = val
>>> frame2
       year   state  pop  debt
one    2000    Ohio  1.5   NaN
two    2001    Ohio  1.7  -1.2
three  2002    Ohio  3.6   NaN
four   2001  Nevada  2.4  -1.5
five   2002  Nevada  2.9  -1.7


为不存在的列赋值会创建出一个新列。关键字del用于删除列：
>>> frame2['eastern'] = frame2.state == 'Ohio'
>>> frame2
       year   state  pop  debt eastern
one    2000    Ohio  1.5   NaN    True
two    2001    Ohio  1.7  -1.2    True
three  2002    Ohio  3.6   NaN    True
four   2001  Nevada  2.4  -1.5   False
five   2002  Nevada  2.9  -1.7   False


另一种常见的数据形式是嵌套字典（也就是字典的字典）
>>> pop = {'Nevada': {2001: 2.4, 2002: 2.9},
... 'Ohio':{2000: 1.5, 2001: 1.7, 2002: 3.6}}
>>> frame3 = DataFrame(pop)
>>> frame3
      Nevada  Ohio
2000     NaN   1.5
2001     2.4   1.7
2002     2.9   3.6

转置
>>> frame3.T
        2000  2001  2002
Nevada   NaN   2.4   2.9
Ohio     1.5   1.7   3.6

由Series组成的字典差不多也是一样的用法：
>>> pdata = {'Ohio':frame3['Ohio'][:-1],
... 'Nevada': frame3['Nevada'][:2]}
>>> DataFrame(pdata)
      Nevada  Ohio
2000     NaN   1.5
2001     2.4   1.7


如果设置了DataFrame的index和columns的name属性，则这些信息也会被显示出来
>>> frame3.index.name = 'year'
>>> frame3.columns.name = 'state'
>>> frame3
state  Nevada  Ohio
year
2000      NaN   1.5
2001      2.4   1.7
2002      2.9   3.6


跟Series一样，values属性也会以二维ndarray的形式返回DataFrame中的数据：
>>> frame3.values
array([[ nan,  1.5],
       [ 2.4,  1.7],
       [ 2.9,  3.6]])
>>> frame2.values
array([[2000, 'Ohio', 1.5, nan, True],
       [2001, 'Ohio', 1.7, -1.2, True],
       [2002, 'Ohio', 3.6, nan, True],
       [2001, 'Nevada', 2.4, -1.5, False],
       [2002, 'Nevada', 2.9, -1.7, False]], dtype=object)



二维ndarry可以输入给DataFrame构造器的数据
>>> xarr = np.array([[1, 2, 3], [2, 4, 6]])
>>> xarr
array([[1, 2, 3],
       [2, 4, 6]])
>>> frame5 = DataFrame(xarr)
>>> frame5
   0  1  2
0  1  2  3
1  2  4  6
>>> frame5 = DataFrame(xarr, columns=['A', 'B','F'], index = ['C', 'D'])
>>> frame5
   A  B  F
C  1  2  3
D  2  4  6

索引对象
>>> obj = Series(range(3), index=['a', 'b', 'c'])
>>> obj
a    0
b    1
c    2
dtype: int64
>>> index = obj.index
>>> index
Index([u'a', u'b', u'c'], dtype='object')
>>> index[1:]
Index([u'b', u'c'], dtype='object')

Index对象是不可修改的
>>> index[1]
'b'
>>> index[1] = 'd'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/linux/centos/7.x/x86_64/pkgs/python/2.7.5/lib64/python2.7/site-packages/pandas/indexes/base.py", line 1237, in __setitem__
    raise TypeError("Index does not support mutable operations")
TypeError: Index does not support mutable operations

不可修改性非常重要，因为这样才能使Index对象在多个数据结构之间安全共享
>>> obj2 = Series([1.5, -2.5, 0], index=index)
>>> obj2
0    1.5
1   -2.5
2    0.0
dtype: float64
>>> obj2.index is index
True


Index的功能也类似一个固定大小的集合
>>> import pandas as pd
>>> from pandas import Series, DataFrame
>>>
>>>
>>> pop = {'Nevada':{2001: 2.4, 2002: 2.9},
... 'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}
>>> frame3 = DataFrame(pop)
>>> frame3
      Nevada  Ohio
2000     NaN   1.5
2001     2.4   1.7
2002     2.9   3.6

>>> pdata={'Ohio':frame3['Ohio'][:-1],	# 到最后个为止
... 'Nevada':frame3['Nevada'][:2]}	# 输出第0个，第1个
>>> DataFrame(pdata)
      Nevada  Ohio
2000     NaN   1.5
2001     2.4   1.7

>>> 'Ohio' in frame3.columns
True
>>> 2003 in frame3.index
False


索引对象






基本功能
重新索引 reindex
>>> obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])
>>> obj
d    4.5
b    7.2
a   -5.3
c    3.6
dtype: float64
>>> obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e']) # 调用该Series的reindex将会根据新索引
>>> obj
d    4.5
b    7.2
a   -5.3
c    3.6
dtype: float64
>>> obj2
a   -5.3
b    7.2
c    3.6
d    4.5
e    NaN
dtype: float64

>>> obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)# 用fill_value来填充值
>>> obj2
a   -5.3
b    7.2
c    3.6
d    4.5
e    0.0
dtype: float64


对于时间序列这样的有序数据，需要插值处理
>>> obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])	
>>> obj3.reindex(range(6), method = 'ffill')	# method选项 可进行插值处理 ffill 前向插值，和前面的一样
0      blue
1      blue
2    purple
3    purple
4    yellow
5    yellow
dtype: object

>>> obj3.reindex(range(5), method = 'bfill')	# bfill 后向引用 和后面的一样
0      blue
1    purple
2    purple
3    yellow
4    yellow


对于DataFrame，reindex可以修改（行）索引、列，或两个都修改。如果仅转入一个序列，则会重新
索引行：
>>> frame = DataFrame(np.arange(9).reshape((3, 3)), index = ['a', 'c', 'd'], columns = ['Ohio', 'Texas', 'California'])
>>> frame
   Ohio  Texas  California
a     0      1           2
c     3      4           5
d     6      7           8


>>> import numpy as np
>>> frame = DataFrame(np.arange(9).reshape((3, 3)), index = ['a', '               umns = ['Ohio', 'Texas', 'California'])
>>> frame
   Ohio  Texas  California
a     0      1           2
c     3      4           5
d     6      7           8
>>> frame2 = frame.reindex(['a', 'b', 'c', 'd'])
>>> frame2
   Ohio  Texas  California
a   0.0    1.0         2.0
b   NaN    NaN         NaN
c   3.0    4.0         5.0
d   6.0    7.0         8.0
>>> states = ['Texas', 'Utah', 'California']
>>> frame.reindex(columns = states)
   Texas  Utah  California
a      1   NaN           2
c      4   NaN           5
d      7   NaN           8

也可以同时对行和列进行重新索引，而插值则只能按行引用（即轴0）：
>>> frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill', columns=states)
   Texas  Utah  California
a      1   NaN           2
b      1   NaN           2
c      4   NaN           5
d      7   NaN           8


利用ix的标签索引功能，重新索引任务可以变得更简洁：
>>> frame.ix[['a', 'b', 'c', 'd'], states]
   Texas  Utah  California
a    1.0   NaN         2.0
b    NaN   NaN         NaN
c    4.0   NaN         5.0
d    7.0   NaN         8.0


丢弃指定轴上的项
>>> obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])
>>> obj
a    0.0
b    1.0
c    2.0
d    3.0
e    4.0
dtype: float64
>>> new_obj = obj.drop('c')	# drop方法返回的是一个在指定轴上删除了指定值的新对象
>>> new_obj
a    0.0
b    1.0
d    3.0
e    4.0
dtype: float64
>>> obj.drop(['d', 'c'])
a    0.0
b    1.0
e    4.0
dtype: float64


对于DataFrame，可以删除任意轴上的索引值：
>>> data = DataFrame(np.arange(16).reshape((4, 4)),
... index=['Ohio', 'Colorado', 'Utah', 'New York'],
... columns=['one', 'two', 'three', 'four'])
>>>
>>> data
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
Utah        8    9     10    11
New York   12   13     14    15

>>> data.drop(['Colorado', 'Ohio'])
          one  two  three  four
Utah        8    9     10    11
New York   12   13     14    15

>>> data.drop('two', axis=1)
          one  three  four
Ohio        0      2     3
Colorado    4      6     7
Utah        8     10    11
New York   12     14    15

>>> data.drop(['two', 'four'], axis=1)	
          one  three
Ohio        0      2
Colorado    4      6
Utah        8     10
New York   12     14


索引、选取和过滤
>>> obj=Series(np.arange(4.), index=['a', 'b', 'c', 'd'])
>>> obj
a    0.0
b    1.0
c    2.0
d    3.0
dtype: float64
>>> obj['b']
1.0
>>> obj[1]
1.0
>>> obj[2:4]
c    2.0
d    3.0
dtype: float64
>>> obj[['b','a','d']]
b    1.0
a    0.0
d    3.0
dtype: float64
>>> obj[[1,3]]
b    1.0
d    3.0
dtype: float64
>>> obj[obj<2]
a    0.0
b    1.0
dtype: float64

利用标签的切片运算与普通的python切片不同，其末端是包含的
>>> obj['b':'c']=5
>>> obj
a    0.0
b    5.0
c    5.0
d    3.0
dtype: float64


>>> data=DataFrame(np.arange(16).reshape((4,4)),
... index=['Ohio', 'Colorado', 'Utah', 'New York'],
... columns=['one','two','three','four'])
>>> data
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
Utah        8    9     10    11
New York   12   13     14    15
>>> data['two']
Ohio         1
Colorado     5
Utah         9
New York    13
Name: two, dtype: int32
>>> data[['three','one']]
          three  one
Ohio          2    0
Colorado      6    4
Utah         10    8
New York     14   12


>>> data=DataFrame(np.arange(16).reshape((4,4)),
... index=['Ohio', 'Colorado', 'Utah', 'New York'],
... columns=['one','two','three','four'])
>>> data
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
Utah        8    9     10    11
New York   12   13     14    15
>>> data['two']	# 选取DataFrame中的列
Ohio         1
Colorado     5
Utah         9
New York    13
Name: two, dtype: int32
>>> data[['three','one']]	# 选取DataFrame中的两列
          three  one
Ohio          2    0
Colorado      6    4
Utah         10    8
New York     14   12
>>> data[:2]			# 选取data中的两行
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
>>> data[data['three']>5]	# 布尔运算
          one  two  three  four
Colorado    4    5      6     7
Utah        8    9     10    11
New York   12   13     14    15


这段代码的目的是使DataFrame在语法上更像ndarray
>>> data<5
            one    two  three   four
Ohio       True   True   True   True
Colorado   True  False  False  False
Utah      False  False  False  False
New York  False  False  False  False
>>>
>>>
>>>
>>> data[data<5]=0
>>> data
          one  two  three  four
Ohio        0    0      0     0
Colorado    0    5      6     7
Utah        8    9     10    11
New York   12   13     14    15


为了在DataFrame的行上进行标签索引，引入专门的索引字段ix

>>> data
          one  two  three  four
Ohio        0    0      0     0
Colorado    0    5      6     7
Utah        8    9     10    11
New York   12   13     14    15
>>> data.ix['Colorado', ['two', 'three']]	# Colorado行， two列和three列
two      5
three    6
Name: Colorado, dtype: int32
>>> data.ix[['Colorado','Utah'],[3, 0, 1]]	# Colorado，Utah行以及3,0,1列
          four  one  two
Colorado     7    0    5
Utah        11    8    9


>>> data
          one  two  three  four
Ohio        0    0      0     0
Colorado    0    5      6     7
Utah        8    9     10    11
New York   12   13     14    15
>>> data.ix[2]					# 选取第三行
one       8
two       9
three    10
four     11
Name: Utah, dtype: int32


>>> data.ix[:'Utah', 'two']			# 从第一行到Utah行的第二列
Ohio        0
Colorado    5
Utah        9


>>> data.ix[data.three > 5, :3]			# three列大于5的行，第一列到第三列
          one  two  three
Colorado    0    5      6
Utah        8    9     10
New York   12   13     14
>>> data
          one  two  three  four
Ohio        0    0      0     0
Colorado    0    5      6     7
Utah        8    9     10    11
New York   12   13     14    15
>>> data.ix[data.three > 5, :1]
          one
Colorado    0
Utah        8
New York   12
>>> data.ix[data.three > 5, :0]
Empty DataFrame
Columns: []
Index: [Colorado, Utah, New York]


算术运算和数据对齐
>>> s1=Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])
>>> s2=Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f' ,'g'])
>>> s1+s2	# 如果直接相加，非重叠部分会显示NaN
a    5.2
c    1.1
d    NaN
e    0.0
f    NaN
g    NaN
dtype: float64
>>> s1.add(s2, fill_value=0)	# add方法以及fill_value参数
a    5.2
c    1.1
d    3.4
e    0.0
f    4.0
g    3.1
dtype: float64


DataFrame和Series之间的运算
>>> arr=np.arange(12.).reshape((3,4))
>>> arr
array([[  0.,   1.,   2.,   3.],
       [  4.,   5.,   6.,   7.],
       [  8.,   9.,  10.,  11.]])
>>> arr[0]
array([ 0.,  1.,  2.,  3.])
>>> arr-arr[0]
array([[ 0.,  0.,  0.,  0.],
       [ 4.,  4.,  4.,  4.],
       [ 8.,  8.,  8.,  8.]])


>>> frame=DataFrame(np.arange(12.).reshape((4,3)),columns=list('bde'),
... index=['Utah', 'Ohio', 'Texas', 'Oregon'])
>>> series=frame.ix[0]
>>> frame
          b     d     e
Utah    0.0   1.0   2.0
Ohio    3.0   4.0   5.0
Texas   6.0   7.0   8.0
Oregon  9.0  10.0  11.0
>>> series
b    0.0
d    1.0
e    2.0
Name: Utah, dtype: float64
>>> frame-series
          b    d    e
Utah    0.0  0.0  0.0
Ohio    3.0  3.0  3.0
Texas   6.0  6.0  6.0
Oregon  9.0  9.0  9.0
>>> series2=Series(range(3), index=['b', 'e', 'f'])
>>> frame+series2
          b   d     e   f
Utah    0.0 NaN   3.0 NaN
Ohio    3.0 NaN   6.0 NaN
Texas   6.0 NaN   9.0 NaN
Oregon  9.0 NaN  12.0 NaN
>>> series3=frame['d']
>>> frame
          b     d     e
Utah    0.0   1.0   2.0
Ohio    3.0   4.0   5.0
Texas   6.0   7.0   8.0
Oregon  9.0  10.0  11.0
>>> series3
Utah       1.0
Ohio       4.0
Texas      7.0
Oregon    10.0
Name: d, dtype: float64
>>> frame.sub(series3)
        Ohio  Oregon  Texas  Utah   b   d   e
Utah     NaN     NaN    NaN   NaN NaN NaN NaN
Ohio     NaN     NaN    NaN   NaN NaN NaN NaN
Texas    NaN     NaN    NaN   NaN NaN NaN NaN
Oregon   NaN     NaN    NaN   NaN NaN NaN NaN
>>> frame.sub(series3, axis=0)	# 匹配行 且在列广播
          b    d    e
Utah   -1.0  0.0  1.0
Ohio   -1.0  0.0  1.0
Texas  -1.0  0.0  1.0
Oregon -1.0  0.0  1.0



###函数应用和映射###

对DataFrame中各个元素，或一行/列数据进行函数操作
>>> frame=DataFrame(np.random.randn(4,3), columns=list('bde'),
... index=['Utah', 'Ohio', 'Texas', 'Oregon'])
>>> frame
               b         d         e
Utah   -1.766152 -2.626653 -0.445074
Ohio   -0.292650  0.801737 -0.572183
Texas  -2.034327 -1.394547 -0.839812
Oregon  0.067651  0.847935 -0.979156
>>> np.abs(frame)	# NumPy的ufuns（元素级数组方法）也可用于操作pandas对象
               b         d         e
Utah    1.766152  2.626653  0.445074
Ohio    0.292650  0.801737  0.572183
Texas   2.034327  1.394547  0.839812
Oregon  0.067651  0.847935  0.979156


>>> f = lambda x: x.max()-x.min()	# DataFrame的apply方法即可实现将函数应用到由
>>> frame.apply(f)			# apply（）各列和行所形成的一维数组上
b    2.101978
d    3.474588
e    0.534083
dtype: float64
>>> frame.apply(f, axis=1)
Utah      2.181580
Ohio      1.373920
Texas     1.194515
Oregon    1.827091
dtype: float64


>>> def f(x):
...     return Series([x.min(), x.max()], index=['min', 'max'])
...
>>> frame.apply(f)			# 传递给apply的函数返回由多个值组成的Series
            b         d         e
min -2.034327 -2.626653 -0.979156
max  0.067651  0.847935 -0.445074


>>> format = lambda x: '%.2f' % x	# 得到frame中各个浮点值的格式化字符串用applymap
>>> frame.applymap(format)
            b      d      e
Utah    -1.77  -2.63  -0.45
Ohio    -0.29   0.80  -0.57
Texas   -2.03  -1.39  -0.84
Oregon   0.07   0.85  -0.98


>>> frame['e'].map(format)	# Series有一个用于应用元素级函数的map方法

Utah      -0.45
Ohio      -0.57
Texas     -0.84
Oregon    -0.98
Name: e, dtype: object


###排序和排名###
>>> obj = Series(range(4), index=['d', 'a', 'b', 'c'])
>>> obj
d    0
a    1
b    2
c    3
dtype: int64

>>> obj.sort_index()
a    1
b    2
c    3
d    0
dtype: int64

>>> frame=DataFrame(np.arange(8).reshape((2,4)), index=['three', 'one'],	# 对于DataFrame，则可以根据任意一个轴上的索引进行排序
... columns=['d', 'a', 'b', 'c'])
>>>
>>>
>>> frame
       d  a  b  c
three  0  1  2  3
one    4  5  6  7
>>> frame.sort_index()
       d  a  b  c
one    4  5  6  7
three  0  1  2  3
>>> frame.sort_index(axis=1)
       a  b  c  d
three  1  2  3  0
one    5  6  7  4


>>> frame.sort_index(axis=1, ascending=False)		# 降序排列
       d  c  b  a
three  0  3  2  1
one    4  7  6  5



>>> obj=Series([4, 7, -3, 2])
>>> obj
0    4
1    7
2   -3
3    2
dtype: int64
>>> obj.order()						# Series排序需要用order
2   -3
3    2
0    4
1    7
dtype: int64
>>> obj.order(ascending=False)
1    7
0    4
3    2
2   -3
dtype: int64


>>> obj = Series([4, np.nan, 7, np.nan, -3, 2])		# 任何缺失值默认都会放到Series的末尾
>>> obj.order()						# 无论是Series还是DataFrame
4   -3.0
5    2.0
0    4.0
2    7.0
1    NaN
3    NaN
dtype: float64
>>> obj.order(ascending=False)
2    7.0
0    4.0
5    2.0
4   -3.0
1    NaN
3    NaN
dtype: float64


>>> frame=DataFrame({'b':[4, 7, -3, 2], 'a':[0,1,0,1]})
>>> frame
   a  b
0  0  4
1  1  7
2  0 -3
3  1  2
>>> frame.sort_index(by='b')	# 根据一个或多个列中的值进行排序。将一个或多个列的名字传递给by选项
   a  b
2  0 -3
3  1  2
0  0  4
1  1  7


>>> frame.sort_index(by=['b','a'])	# 先排b列 再排a列
   a  b
2  0 -3
3  1  2
0  0  4
1  1  7
>>> frame.sort_index(by=['a','b'])	# 先排a列 再排b列
   a  b
2  0 -3
0  0  4
3  1  2
1  1  7


排名（ranking）跟排序关系密切
>>> obj=Series([1,1,1,1,1,1])	# 从1开始，然后相同的数取平均数。
>>> obj.rank()
0    3.5
1    3.5
2    3.5
3    3.5
4    3.5
5    3.5
dtype: float64

>>> obj.rank()			# 从1开始，然后相同的数取平均数。
0    6.5
1    1.0
2    6.5
3    4.5
4    3.0
5    2.0
6    4.5
dtype: float64
>>> obj.rank(method='first')	#根据值在原数据中出现的顺序给出排名
0    6.0
1    1.0
2    7.0
3    4.0
4    3.0
5    2.0
6    5.0
dtype: float64
>>> obj.rank(ascending=False, method='max')	# 降序排名 max使用分组中的最大排名
0    2.0
1    7.0
2    2.0
3    4.0
4    5.0
5    6.0
6    4.0
dtype: float64

>>> frame= DataFrame({'b':[4.3, 7, -3, 2], 'a':[0, 1, 0, 1], 'c':[-2, 5, 8, -2.5]})
>>> frame
   a    b    c
0  0  4.3 -2.0
1  1  7.0  5.0
2  0 -3.0  8.0
3  1  2.0 -2.5
>>> frame.rank(axis=1)
     a    b    c
0  2.0  3.0  1.0
1  1.0  3.0  2.0
2  2.0  1.0  3.0
3  2.0  3.0  1.0


###带有重复值的轴索引###
>>> obj=Series(range(5), index=['a', 'a', 'b', 'b', 'c'])
>>> obj
a    0
a    1
b    2
b    3
c    4
dtype: int64
>>> obj.index.is_unique
False

>>> obj['a']		# 如果某个索引对应多个值，则返回一个Series
a    0
a    1
dtype: int64
>>> obj['c']		# 对应单个值，则返回一个标量值
4

>>> df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])
>>> df
          0         1         2
a -0.761022  0.228550 -1.123073
a  0.668955 -1.197281 -0.328058
b -0.038427  2.159243  0.187048
b -0.588112  0.232811  0.859724
>>> df.ix['b']
          0         1         2
b -0.038427  2.159243  0.187048
b -0.588112  0.232811  0.859724


汇总和计算描述统计
>>> from pandas import DataFrame
>>> import numpy as np
>>> df=DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]],
... index=['a', 'b', 'c', 'd'], columns=['one', 'two'])
>>> df
    one  two
a  1.40  NaN
b  7.10 -4.5
c   NaN  NaN
d  0.75 -1.3
>>> df.sum()	# 按列进行求和
one    9.25
two   -5.80
dtype: float64
>>> df.sum(axis=1)	# 按行进行求和
a    1.40
b    2.60
c     NaN
d   -0.55
dtype: float64


>>> df
    one  two
a  1.40  NaN
b  7.10 -4.5
c   NaN  NaN
d  0.75 -1.3
>>> df.mean(axis=1, skipna=False)		# 禁止对于Na值的排除
a      NaN
b    1.300
c      NaN
d   -0.275
dtype: float64
>>> df.mean(axis=1)				# 
a    1.400
b    1.300
c      NaN
d   -0.275
dtype: float64


>>> df				# 返回间接统计（比如达到最小值或最大值的索引）
    one  two
a  1.40  NaN
b  7.10 -4.5
c   NaN  NaN
d  0.75 -1.3

>>> df.idxmax()
one    b
two    d
dtype: object

>>> df.idxmax()			# 累加
one    b
two    d
dtype: object
>>> df.cumsum()
    one  two
a  1.40  NaN
b  8.50 -4.5
c   NaN  NaN
d  9.25 -5.8


相关系数和协方差
>>> import pandas.io.data as web
>>> from pandas import Series
>>> from pandas import DataFrame
>>> all_data={}			# 创建一个空字典
>>> for ticker in ['AAPL', 'IBM']:
...     all_data[ticker]=web.get_data_yahoo(ticker, '1/1/2000', '1/10/2000')	# 从yahoofinance中提取AAPL和IBM的股票
>>> all_data									# all_data字典中key是'AAPL' 或'IBM' value是一个DataFrame的数据结构
{'AAPL':                   Open        High         Low       Close     Volume  \
Date
2000-01-03  104.874997  112.499998  101.687501  111.937502  133949200
2000-01-04  108.250001  110.625002  101.187503  102.500003  128094400
2000-01-05  103.749998  110.562497  103.000001  103.999997  194580400
2000-01-06  106.124999  106.999999   94.999998   94.999998  191993200
2000-01-07   96.499999  101.000002   95.500003   99.500001  115183600
2000-01-10  101.999998  102.249997   94.749999   97.750001  126266000

 Adj Close
Date
2000-01-03   3.641362
2000-01-04   3.334358
2000-01-05   3.383153
2000-01-06   3.090380
2000-01-07   3.236767

>>> price=DataFrame({tic:data['Adj Close'] for tic, data in all_data.iteritems()})	# iteritems 2.X python有itervalues()这一组函数 
>>> price										# 用于返回一个 iterator 
                AAPL        IBM
Date
2000-01-03  3.641362  88.455060
2000-01-04  3.334358  85.452544
2000-01-05  3.383153  88.455060
2000-01-06  3.090380  86.929973
2000-01-07  3.236767  86.548701
2000-01-10  3.179838  89.980147


>>> returns=price.pct_change()			# 计算百分数变化
>>> returns
                AAPL       IBM
Date
2000-01-03       NaN       NaN
2000-01-04 -0.084310 -0.033944
2000-01-05  0.014634  0.035137
2000-01-06 -0.086539 -0.017241
2000-01-07  0.047369 -0.004386
2000-01-10 -0.017588  0.039648
>>> price
                AAPL        IBM
Date
2000-01-03  3.641362  88.455060
2000-01-04  3.334358  85.452544
2000-01-05  3.383153  88.455060
2000-01-06  3.090380  86.929973
2000-01-07  3.236767  86.548701
2000-01-10  3.179838  89.980147
>>> returns.tail()				# 无NaN行
                AAPL       IBM
Date
2000-01-04 -0.084310 -0.033944
2000-01-05  0.014634  0.035137
2000-01-06 -0.086539 -0.017241
2000-01-07  0.047369 -0.004386
2000-01-10 -0.017588  0.039648


>>> returns.AAPL.std()
0.059513194146709428
>>> returns.IBM.std()
0.032409034043103566
>>> returns.AAPL.corr(returns.IBM)
0.57663870858430299
>>> returns.AAPL.cov(returns.IBM)
0.0011122006366748758
>>> returns.IBM.std()*returns.AAPL.std()*returns.AAPL.corr(returns.IBM)	# 两列数的协方差等于两个标准差乘以相关系数
0.0011122006366748756


>>> returns.corr()							# DataFrame的corr和cov返回完整的相关系数和协方差矩阵
          AAPL       IBM
AAPL  1.000000  0.576639
IBM   0.576639  1.000000
>>> returns.cov()
          AAPL       IBM
AAPL  0.003542  0.001112
IBM   0.001112  0.001050


>>> returns.corrwith(returns.IBM)					# corrwith可以计算一个DataFrame和另一个series和DataFrame之间的相关系数
AAPL    0.576639
IBM     1.000000
dtype: float64


唯一值、值计数以及成员资格
>>> obj=Series(['c','a','d','a','a','b','b','c','c'])	# 第一个函数是unique
>>> uniques=obj.unique()				# 它可以得到Series中的唯一值数组
>>> uniques
array(['c', 'a', 'd', 'b'], dtype=object)


>>> uniques = obj.unique()
>>> uniques
array(['c', 'a', 'd', 'b'], dtype=object)
>>> obj.value_counts()	# value_counts用于计算一个Series中各值出现的频率
c    3
a    3
b    2
d    1
dtype: int64


>>> mask=obj.isin(['b', 'c'])		# 用于判断矢量化集合的成员资格
>>> mask				# 可用于选取Series中或DataFrame列中数据的子集
0     True
1    False
2    False
3    False
4    False
5     True
6     True
7     True
8     True
dtype: bool
>>> obj[mask]
0    c
5    b
6    b
7    c
8    c
dtype: object


>>> data=DataFrame({'Qu1':[1, 3, 4, 3, 4],
... 'Qu2':[2, 3, 1, 2, 3],
... 'Qu3':[1, 5, 2, 4, 4]})
>>> data
   Qu1  Qu2  Qu3
0    1    2    1
1    3    3    5
2    4    1    2
3    3    2    4
4    4    3    4
>>> result=data.apply(pd.value_counts).fillna(0)	# 对data中的列apply pd.value_counts函数
>>> result
   Qu1  Qu2  Qu3
1  1.0  1.0  1.0
2  0.0  2.0  1.0
3  2.0  2.0  0.0
4  2.0  0.0  2.0
5  0.0  0.0  1.0

>>> result=data.apply(pd.value_counts, axis=1).fillna(0)
>>> result
     1    2    3    4    5
0  2.0  1.0  0.0  0.0  0.0
1  0.0  0.0  2.0  0.0  1.0
2  1.0  1.0  0.0  1.0  0.0
3  0.0  1.0  1.0  1.0  0.0
4  0.0  0.0  1.0  2.0  0.0


处理缺失数据
>>> string_data=Series(['aardvark', 'artichoke', np.nan, 'avocado'])		# NaN表示浮点和非浮点数组中的缺失数据
>>> string_data
0     aardvark
1    artichoke
2          NaN
3      avocado
dtype: object
>>> string_data.isnull()
0    False
1    False
2     True
3    False
dtype: bool
>>> string_data[0]=None
>>> string_data
0         None
1    artichoke
2          NaN
3      avocado
dtype: object
>>> string_data.isnull()
0     True
1    False
2     True
3    False
dtype: bool
>>>


滤除缺失数据
>>> from numpy import nan as NA
>>> data=Series([1, NA, 3.5, NA, 7])
>>> data.dropna()
0    1.0
2    3.5
4    7.0
dtype: float64

>>> data[data.notnull()]	# 也可以通过布尔型索引达到这个目的
0    1.0
2    3.5
4    7.0
dtype: float64


>>> data=DataFrame([[1.0, 6.5, 3.0], [1.0, NA, NA],
... [NA, NA, NA], [NA, 6.5, 3.0]])
>>> data
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
2  NaN  NaN  NaN
3  NaN  6.5  3.0
>>> cleaned=data.dropna()		# dropna默认丢弃任何有缺失值的行
>>> cleaned
     0    1    2
0  1.0  6.5  3.0


>>> data.dropna(how='all')		# how=‘all’将只丢弃全为NA的那些行
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
3  NaN  6.5  3.0


>>> data
     0    1    2   4
0  1.0  6.5  3.0 NaN
1  1.0  NaN  NaN NaN
2  NaN  NaN  NaN NaN
3  NaN  6.5  3.0 NaN
>>> data.dropna(axis=1,how='all')		# 丢弃整列都是NA值
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
2  NaN  NaN  NaN
3  NaN  6.5  3.0


>>> df=DataFrame(np.random.randn(7,3))
>>> df.ix[:4, 1]=NA
>>> df.ix[:2, 2]=NA
>>> df
          0         1         2
0 -0.842856       NaN       NaN
1 -0.862389       NaN       NaN
2  1.149765       NaN       NaN
3  0.520349       NaN -1.481892
4 -1.130664       NaN -0.200879
5  0.350709 -0.828031 -0.507767
6 -0.358415  0.606560  1.027162
>>> df.dropna(thresh=3)		# 至少每行有三个非NA
          0         1         2
5  0.350709 -0.828031 -0.507767
6 -0.358415  0.606560  1.027162
>>> df.dropna(thresh=1)
          0         1         2
0 -0.842856       NaN       NaN
1 -0.862389       NaN       NaN
2  1.149765       NaN       NaN
3  0.520349       NaN -1.481892
4 -1.130664       NaN -0.200879
5  0.350709 -0.828031 -0.507767
6 -0.358415  0.606560  1.027162
>>> df.dropna(thresh=2)
          0         1         2
3  0.520349       NaN -1.481892
4 -1.130664       NaN -0.200879
5  0.350709 -0.828031 -0.507767
6 -0.358415  0.606560  1.027162


填充缺失数据
>>> df.fillna(0)
          0         1         2
0 -0.842856  0.000000  0.000000
1 -0.862389  0.000000  0.000000
2  1.149765  0.000000  0.000000
3  0.520349  0.000000 -1.481892
4 -1.130664  0.000000 -0.200879
5  0.350709 -0.828031 -0.507767
6 -0.358415  0.606560  1.027162

>>> df.fillna({1:0.5, 2:-1})		# 通过一个字典调用fillna，就可以实现对不同的列填充不同的值
          0         1         2
0 -0.842856  0.500000 -1.000000
1 -0.862389  0.500000 -1.000000
2  1.149765  0.500000 -1.000000
3  0.520349  0.500000 -1.481892
4 -1.130664  0.500000 -0.200879
5  0.350709 -0.828031 -0.507767
6 -0.358415  0.606560  1.027162

>>> _=df.fillna(0, inplace=True)		# fillna返回新对象，但也可以对现有对象进行就地修改
>>> df
          0         1         2
0 -0.842856  0.000000  0.000000
1 -0.862389  0.000000  0.000000
2  1.149765  0.000000  0.000000
3  0.520349  0.000000 -1.481892
4 -1.130664  0.000000 -0.200879
5  0.350709 -0.828031 -0.507767
6 -0.358415  0.606560  1.027162

>>> df = DataFrame(np.random.randn(6, 3))
>>> df.ix[2:, 1]=NA
>>> df.ix[4:, 2]=NA
>>> df
          0         1         2
0 -0.601720  0.868540  0.730089
1 -0.585113  1.390372  1.030539
2  1.208324       NaN  1.434392
3 -2.173415       NaN -0.340178
4 -0.028685       NaN       NaN
5  0.384403       NaN       NaN
>>> df.fillna(method='ffill')
          0         1         2
0 -0.601720  0.868540  0.730089
1 -0.585113  1.390372  1.030539
2  1.208324  1.390372  1.434392
3 -2.173415  1.390372 -0.340178
4 -0.028685  1.390372 -0.340178
5  0.384403  1.390372 -0.340178
>>> df.fillna(method='ffill', limit=2)
          0         1         2
0 -0.601720  0.868540  0.730089
1 -0.585113  1.390372  1.030539
2  1.208324  1.390372  1.434392
3 -2.173415  1.390372 -0.340178
4 -0.028685       NaN -0.340178
5  0.384403       NaN -0.340178

>>> data=Series([1.0, NA, 3.5, NA, 7])
>>> data.fillna(data.mean())
0    1.000000
1    3.833333
2    3.500000
3    3.833333
4    7.000000
dtype: float64


层次化索引

>>> import  numpy as np
>>> import pandas as pd
>>> from pandas import Series
>>> from pandas import DataFrame
>>> data=Series(np.random.randn(10),
... index=[['a','a','a','b','b','b','c','c','d','d'],
... [1,2,3,1,2,3,1,2,2,3]])
>>> data
a  1   -0.386136
   2   -1.933462
   3    1.564141
b  1   -0.031232
   2   -0.305060
   3    0.524174
c  1    3.111645
   2    0.247871
d  2    0.722982
   3   -0.256762
dtype: float64

>>> data.index
MultiIndex(levels=[[u'a', u'b', u'c', u'd'], [1, 2, 3]],
           labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2]])


>>> data['b']		# 对于层次化索引对象，选取数据子集
1   -0.031232
2   -0.305060
3    0.524174
dtype: float64


>>> data['b':'c']
b  1   -0.031232
   2   -0.305060
   3    0.524174
c  1    3.111645
   2    0.247871
dtype: float64
>>> data.ix[['b','d']]
b  1   -0.031232
   2   -0.305060
   3    0.524174
d  2    0.722982
   3   -0.256762
dtype: float64
>>> data['b':'d']
b  1   -0.031232
   2   -0.305060
   3    0.524174
c  1    3.111645
   2    0.247871
d  2    0.722982
   3   -0.256762
dtype: float64

>>> data[:,2]		# 在“内层”中进行选取
a   -1.933462
b   -0.305060
c    0.247871
d    0.722982
dtype: float64


>>> data.unstack()		#通过unstack方法重新安排到一个DataFrame中
          1         2         3
a -0.386136 -1.933462  1.564141
b -0.031232 -0.305060  0.524174
c  3.111645  0.247871       NaN
d       NaN  0.722982 -0.256762


>>> data.unstack().stack()		# unstack的逆运算是stack
a  1   -0.386136
   2   -1.933462
   3    1.564141
b  1   -0.031232
   2   -0.305060
   3    0.524174
c  1    3.111645
   2    0.247871
d  2    0.722982
   3   -0.256762
dtype: float64


>>> frame=DataFrame(np.arange(12).reshape((4,3)),index=[['a','a','b','b'],[1, 2, 1, 2]],
... columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])
>>> frame		# 一个DataFrame， 每条轴都可以有分层索引
     Ohio     Colorado
    Green Red    Green
a 1     0   1        2
  2     3   4        5
b 1     6   7        8
  2     9  10       11


>>> frame.index.names=['key1', 'key2']		# 索引名称
>>> frame.columns.names=['state', 'color']
>>> frame
state      Ohio     Colorado
color     Green Red    Green
key1 key2
a    1        0   1        2
     2        3   4        5
b    1        6   7        8
     2        9  10       11

>>> frame['Ohio']
color      Green  Red
key1 key2
a    1         0    1
     2         3    4
b    1         6    7
     2         9   10


###重排分级顺序###
>>> frame
state      Ohio     Colorado
color     Green Red    Green
key1 key2
a    1        0   1        2
     2        3   4        5
b    1        6   7        8
     2        9  10       11
>>> frame.swaplevel('key1', 'key2')	# swaplevel接受两个级别编号或名称，并返回一个
state      Ohio     Colorado		# 互换了级别的新对象（但数据不会发生变化）
color     Green Red    Green
key2 key1
1    a        0   1        2
2    a        3   4        5
1    b        6   7        8
2    b        9  10       11

swaplevel的应用
>>> frame1
state            Ohio            Colorado
color           Green       Red     Green
keys1 keys2
a     3     -0.026843 -0.557449  2.983456
      1     -0.303709  0.950943 -1.699359
      2      0.201512  0.610445  0.289560
c     1      0.690963  1.475376 -1.451312
b     1     -0.422497  0.284796 -0.990737

>>> frame1.swaplevel('keys1', 'keys2')
state            Ohio            Colorado
color           Green       Red     Green
keys2 keys1
3     a     -0.026843 -0.557449  2.983456
1     a     -0.303709  0.950943 -1.699359
2     a      0.201512  0.610445  0.289560
1     c      0.690963  1.475376 -1.451312
      b     -0.422497  0.284796 -0.990737

>>> frame1
state            Ohio            Colorado
color           Green       Red     Green
keys1 keys2
a     3     -0.026843 -0.557449  2.983456
      1     -0.303709  0.950943 -1.699359
      2      0.201512  0.610445  0.289560
c     1      0.690963  1.475376 -1.451312
b     1     -0.422497  0.284796 -0.990737

>>> frame1.sortlevel(0)				# sortlevel根据单个级别中的值对数据进行排序
state            Ohio            Colorado
color           Green       Red     Green
keys1 keys2
a     1     -0.303709  0.950943 -1.699359
      2      0.201512  0.610445  0.289560
      3     -0.026843 -0.557449  2.983456
b     1     -0.422497  0.284796 -0.990737
c     1      0.690963  1.475376 -1.451312

>>> frame1.sortlevel(1)
state            Ohio            Colorado
color           Green       Red     Green
keys1 keys2
a     1     -0.303709  0.950943 -1.699359
b     1     -0.422497  0.284796 -0.990737
c     1      0.690963  1.475376 -1.451312
a     2      0.201512  0.610445  0.289560
      3     -0.026843 -0.557449  2.983456

###根据级别汇总统计###

我们可以根据行或列上的级别来进行求和
>>> frame
state      Ohio     Colorado
color     Green Red    Green
key1 key2
a    1        0   1        2
     2        3   4        5
b    1        6   7        8
     2        9  10       11
>>> frame.sum(level='key2')	# 按行进行求和，按key2来求和，相同key2结合在一起
state  Ohio     Colorado
color Green Red    Green
key2
1         6   8       10
2        12  14       16
>>> frame.sum(level='color', axis=1)	# 按列中的color进行求和
color      Green  Red
key1 key2
a    1         2    1
     2         8    4
b    1        14    7
     2        20   10

###使用DataFrame的列###

人们经常想要将DataFrame的一个或者多个列当做行索引来用
或者可能希望将行索引变成DataFrame的列。

>>> frame=DataFrame({'a':range(7), 'b':range(7, 0, -1),
... 'c':['one', 'one', 'one', 'two', 'two', 'two', 'two'],
... 'd':[0, 1, 2, 0, 1, 2, 3]})
>>> frame
   a  b    c  d
0  0  7  one  0
1  1  6  one  1
2  2  5  one  2
3  3  4  two  0
4  4  3  two  1
5  5  2  two  2
6  6  1  two  3
>>> frame.loc[0:3]	# 行索引将
   a  b    c  d
0  0  7  one  0
1  1  6  one  1
2  2  5  one  2
3  3  4  two  0
>>> frame[0:2]		# 行索引
   a  b    c  d
0  0  7  one  0
1  1  6  one  1
>>> frame[['a','b']]	# 列索引
   a  b
0  0  7
1  1  6
2  2  5
3  3  4
4  4  3
5  5  2
6  6  1

>>> frame2=frame.set_index(['c', 'd'])		# set_index函数会将其一个或多个列转换为行索引，并创建一个新的DataFrame 
>>> frame2
       a  b
c   d
one 0  0  7
    1  1  6
    2  2  5
two 0  3  4
    1  4  3
    2  5  2
    3  6  1

>>> frame.set_index(['c', 'd'], drop=False)	# 那些列会从DataFrame中移除，但也可以将其保留
       a  b    c  d
c   d
one 0  0  7  one  0
    1  1  6  one  1
    2  2  5  one  2
two 0  3  4  two  0
    1  4  3  two  1
    2  5  2  two  2
    3  6  1  two  3

>>> frame2.reset_index()	# reset_index的功能跟set_index刚好相反，层次化
     c  d  a  b			# 索引的级别会被转移到列里面
0  one  0  0  7
1  one  1  1  6
2  one  2  2  5
3  two  0  3  4
4  two  1  4  3
5  two  2  5  2
6  two  3  6  1


###其他有关pandas的话题###

整数索引
>>> ser=Series(np.arange(3.))
>>> ser
0    0.0
1    1.0
2    2.0
dtype: float64
>>> ser[-1]	# 直接整数索引会有bug
KeyError: -1L
>>> ser2=Series(np.arange(3.), index=['a', 'b', 'c'])
>>> ser2[-1]
2.0

>>> ser.ix[::-1]		# 如果你的轴索引含有索引器 ix进行切片
2    2.0
1    1.0
0    0.0


>>> ser3=Series(range(3), index=[-5, 1, 3])
>>> ser3.iget_value(2)		# Series使用iget_value方法
2
>>> frame=DataFrame(np.arange(6).reshape(3, 2), index=[2, 0, 1])
>>> frame.irow(0)		# frame.irow返回的是一个Series
__main__:1: FutureWarning: irow(i) is deprecated. Please use .iloc[i]
0    0
1    1
Name: 2, dtype: int32
>>> frame.irow(0).icol(0)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "D:\python\pandas\core\generic.py", line 2672, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Series' object has no attribute 'icol'
>>> frame.irow(0).iget_value(0)		# 第一列第一行
0

###面板数据###
>>> pdata=pd.Panel(dict((stk, web.get_data_yahoo(stk, '1/1/2009', '6/1/2012'))		#Panel可以看做一个三维版的DataFrame
... for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL']))
>>> pdata
<class 'pandas.core.panel.Panel'>
Dimensions: 4 (items) x 868 (major_axis) x 6 (minor_axis)
Items axis: AAPL to MSFT
Major_axis axis: 2009-01-02 00:00:00 to 2012-06-01 00:00:00
Minor_axis axis: Open to Adj Close

>>> pdata=pdata.swapaxes('items', 'minor')		# 两个坐标轴互换
>>> pdata
<class 'pandas.core.panel.Panel'>
Dimensions: 6 (items) x 868 (major_axis) x 4 (minor_axis)
Items axis: Open to Adj Close
Major_axis axis: 2009-01-02 00:00:00 to 2012-06-01 00:00:00
Minor_axis axis: AAPL to MSFT

>>> pdata.ix['Adj Close', '5/28/2012':'6/1/2012', :]	# 指定每个维度的值以及指定日期或日期范围
                 AAPL      DELL        GOOG       MSFT
Date
2012-05-28        NaN  12.05319         NaN        NaN
2012-05-29  74.464493  12.24666  296.873645  26.072491
2012-05-30  75.362333  12.14992  293.821674  25.878448
2012-05-31  75.174961  11.92743  290.140354  25.746145
2012-06-01  72.996726  11.67592  285.205295  25.093451

>>> stacked=pdata.ix[:,'5/30/2012':, :].to_frame()	# 通过to_frame形成堆积式的DataFrame
>>> stacked
                        Open        High         Low       Close       Volume  \
Date       minor
2012-05-30 AAPL   569.199997  579.989990  566.559990  579.169998  132357400.0
           DELL    12.590000   12.700000   12.460000   12.560000   19787800.0
           GOOG   588.161028  591.901014  583.530999  588.230992    3827600.0
           MSFT    29.350000   29.480000   29.120001   29.340000   41585500.0
2012-05-31 AAPL   580.740021  581.499985  571.460022  577.730019  122918600.0
           DELL    12.530000   12.540000   12.330000   12.330000   19955600.0
           GOOG   588.720982  590.001032  579.001013  580.860990    5958800.0
           MSFT    29.299999   29.420000   28.940001   29.190001   39134000.0
2012-06-01 AAPL   569.159996  572.650009  560.520012  560.989983  130246900.0
           DELL    12.150000   12.300000   12.045000   12.070000   19397600.0
           GOOG   571.790972  572.650996  568.350996  570.981000    6138700.0
           MSFT    28.760000   28.959999   28.440001   28.450001   56634300.0

                   Adj Close
Date       minor
2012-05-30 AAPL    75.362333
           DELL    12.149920
           GOOG   293.821674
           MSFT    25.878448
2012-05-31 AAPL    75.174961
           DELL    11.927430
           GOOG   290.140354
           MSFT    25.746145
2012-06-01 AAPL    72.996726
           DELL    11.675920
           GOOG   285.205295
           MSFT    25.093451

>>> stacked.to_panel()		# to_frame的逆运算
<class 'pandas.core.panel.Panel'>
Dimensions: 6 (items) x 3 (major_axis) x 4 (minor_axis)
Items axis: Open to Adj Close
Major_axis axis: 2012-05-30 00:00:00 to 2012-06-01 00:00:00
Minor_axis axis: AAPL to MSFT



########################################
########数据加载，存储与文件格式########
########################################

###读写文本格式的数据###
PS C:\Users\Weihong> type ex1.csv
a,b,c,d,message
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,foo

>>> df=pd.read_csv('ex1.csv')	# 用read_csv将其读入一个DataFrame
>>> df
   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo

>>> pd.read_table('ex1.csv', sep=',')	# 用read_table只不过需要指定分隔符
   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo


>>> df=pd.read_csv('ex2.csv')		# 如果不设定，read_csv会把第一行默认是column name
>>> df
   1   2   3   4  hello
0  5   6   7   8  world
1  9  10  11  12    foo
>>> df=pd.read_csv('ex2.csv', header=None)	# 自动加 0 1 2.。。
>>> df
   0   1   2   3      4
0  1   2   3   4  hello
1  5   6   7   8  world
2  9  10  11  12    foo

>>> df=pd.read_csv('ex2.csv', names=['a', 'b', 'c', 'd', 'message'])	# 命名每列的名字
>>> df
   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo

>>> names=['a', 'b', 'c', 'd', 'message']
>>> pd.read_csv('ex2.csv', names=names, index_col='message')		# 希望将message列做成DataFrame的索引d
         a   b   c   d
message
hello    1   2   3   4
world    5   6   7   8
foo      9  10  11  12


>>> parsed=pd.read_csv('csv_mindex.csv', index_col=['key1', 'key2'])	# 将多个列做成一个层次化索引
>>> parsed								# 只需传入由列编号或列名组成的列表即可
           value1  value2
key1 key2
one  a          1       2
     b          3       4
     c          5       6
     d          7       8
two  a          9      10
     b         11      12



	A	B	C		# 文件各个字段由数量不定的空白分隔符
aaa   -0.264  -1.026   -0.619		# 可以用正则表达式\s+表示
bbb    0.927   0.302    0.0323		
ccc    0.264   0.386   -0.217
ddd    -0.871  0.348    1.1004

>>> list(open('ex3.txt'))
['\tA\tB\tC\n', 'aaa   -0.264  -1.026   -0.619\n',
, 'ddd    -0.871  0.348    1.1004']
>>> result=pd.read_table('ex3.txt', sep='\s+')	#\s+表示一个或多个空白
>>> result					# 由于列名比数据行的数量少，所以read_table腿短第一列是DataFrame的索引
         A      B       C
aaa -0.264 -1.026 -0.6190
bbb  0.927  0.302  0.0323
ccc  0.264  0.386 -0.2170
ddd -0.871  0.348  1.1004


#hey
a,b,c,d,message
#just wanted 
#who reads CSV
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,foo
>>> pd.read_csv('ex4.csv', skiprows=[0, 2, 3])		# 跳过文件的第一行、第三行和第四行
   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo


>>> result=pd.read_csv('ex5.csv')
>>> result
  something  a   b     c   d message
0       one  1   2   3.0   4     NaN
1       two  5   6   NaN   8   world
2     three  9  10  11.0  12     foo
>>> result=pd.read_csv('ex5.csv', na_values=['NULL'])
>>> result
  something  a   b     c   d message
0       one  1   2   3.0   4     NaN
1       two  5   6   NaN   8   world
2     three  9  10  11.0  12     foo

>>> result=pd.read_csv('ex5.csv', na_values=['one'])	# 指定某个值变成NaN
>>> result
  something  a   b     c   d message
0       NaN  1   2   3.0   4     NaN
1       two  5   6   NaN   8   world
2     three  9  10  11.0  12     foo


###逐块读取文本文件###
>>> result=pd.read_csv('ex6.csv', nrows=5)	# 通过nrows进行指定即可
>>> result
        one       two     three      four key
0  0.467976 -0.038649 -0.295344 -1.824726   L
1 -0.358893  1.404453  0.704965 -0.200638   B
2 -0.501840  0.659254 -0.421691 -0.057688   G
3  0.204886  1.074134  1.388361 -0.982404   R
4  0.354628 -0.133116  0.283763 -0.837063   Q


>>> chunker=pd.read_csv('ex6.csv', chunksize=1000)	# read_csv所返回的这个对象使我们可以根据chunksize对文件逐块迭代
>>> chunker
<pandas.io.parsers.TextFileReader object at 0x05637CF0>
>>> chunker=pd.read_csv('ex6.csv', chunksize=10)
>>> for piece in chunker:				# 每次读十行
...     print piece
...
        one       two     three      four key
0  0.467976 -0.038649 -0.295344 -1.824726   L
1 -0.358893  1.404453  0.704965 -0.200638   B
2 -0.501840  0.659254 -0.421691 -0.057688   G
3  0.204886  1.074134  1.388361 -0.982404   R
4  0.354628 -0.133116  0.283763 -0.837063   Q
5  1.817480  0.742273  0.419395 -2.251035   Q
6 -0.776764  0.935518 -0.332872 -1.875641   U
7 -0.913135  1.530624 -0.572657  0.477252   K
8  0.358480 -0.497572 -0.367016  0.507702   S
9 -1.740877 -1.160417 -1.637830  2.172201   G
        one       two     three      four key
0  0.240564 -0.328249  1.252155  1.072796   8
1  0.764018  1.165476 -0.639544  1.495258   R
2  0.571035 -0.310537  0.582437 -0.298765   1
3  2.317658  0.430710 -1.334216  0.199679   P
4  1.547771 -1.119753 -2.277634  0.329586   J
5 -1.310608  0.401719 -1.000987  1.156708   E
6 -0.088496  0.634712  0.153324  0.415335   B
7 -0.018663 -0.247487 -1.446522  0.750938   A
8 -0.070127 -1.579097  0.120892  0.671432   F
9 -0.194678 -0.492039  2.359605  0.319810   H

>>> tot=Series([])
>>> for piece in chunker:
...     tot=tot.add(piece['key'].value_counts(), fill_value=0)		#piece['key']得到的一个Series，因此可以用value_counts()
...
>>> tot=tot.order(ascending=False)
__main__:1: FutureWarning: order is deprecated, use sort_values(...)
>>> tot[:10]								#value_counts计数
E    368.0
X    364.0
L    346.0
O    343.0
Q    340.0
M    338.0
J    337.0
F    335.0
K    334.0
H    330.0
dtype: float64


###将数据写到文本格式###
>>> data=pd.read_csv('ex5.csv')
>>> data
  something  a   b     c   d message
0       one  1   2   3.0   4     NaN
1       two  5   6   NaN   8   world
2     three  9  10  11.0  12     foo

>>> data.to_csv('out.csv')		# 我们将数据输出写到一个以逗号分隔的文件中

,something,a,b,c,d,message		# 缺失值会表示空字符串
0,one,1,2,3.0,4,
1,two,5,6,,8,world
2,three,9,10,11.0,12,foo

>>> data.to_csv(sys.stdout, sep='|')	#使用其他分隔符，这里直接写到sys.stdout
|something|a|b|c|d|message		# 所以仅仅是打印出文本结果而已
0|one|1|2|3.0|4|
1|two|5|6||8|world
2|three|9|10|11.0|12|foo


>>> data.to_csv(sys.stdout, na_rep=7)	# 可以将空字符表示为别的标记值
,something,a,b,c,d,message
0,one,1,2,3.0,4,7
1,two,5,6,7,8,world
2,three,9,10,11.0,12,foo

>>> data.to_csv(sys.stdout, index=False, header=False)	# 行和列的标签都可以被禁用
one,1,2,3.0,4,
two,5,6,,8,world
three,9,10,11.0,12,foo


>>> dates=pd.date_range('1/1/2000', periods=7)
>>> dates
DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',
               '2000-01-05', '2000-01-06', '2000-01-07'],
              dtype='datetime64[ns]', freq='D')
>>> ts=Series(np.arange(7), index=dates)
>>> ts
2000-01-01    0
2000-01-02    1
2000-01-03    2
2000-01-04    3
2000-01-05    4
2000-01-06    5
2000-01-07    6
Freq: D, dtype: int32
>>> ts.to_csv('ts_series.csv')	# Series也有一个to_csv方法 输出的方法

ts_series.csv
2000-01-01,0
2000-01-02,1
2000-01-03,2
2000-01-04,3
2000-01-05,4
2000-01-06,5
2000-01-07,6

>>> Series.from_csv('ts_series.csv', parse_dates=True)		# 能用from_csv将CSV文件读取为Series
2000-01-01    0
2000-01-02    1
2000-01-03    2
2000-01-04    3
2000-01-05    4
2000-01-06    5
2000-01-07    6
dtype: int64


###手工处理分隔符格式###

>>> obj="""		# JSON数据
... {"name": "Wes",
... "places_lived":["United States", "Spain", "Germany"],
... "pet":null,
... "siblings":[{"name":"Scott", "age":25, "pet": "Zuko"},
... {"name":"Katie", "age":33, "pet":"Cisco"}]
... }
...
... """
>>> obj
'\n{"name": "Wes",\n"places_lived":["United States", "Spain", "Germany"],\n"pet":null,\n"sib
e":25, "pet": "Zuko"},\n{"name":"Katie", "age":33, "pet":"Cisco"}]\n}\n\n'
>>> import json
>>> result=json.loads(obj)
>>> result
{u'pet': None, u'siblings': [{u'pet': u'Zuko', u'age': 25, u'name': u'Scott'}, {u'pet': u'Ci
'Katie'}], u'name': u'Wes', u'places_lived': [u'United States', u'Spain', u'Germany']}
>>> asjson=json.dumps(result)		# python格式转为JSON格式

>>> siblings=DataFrame(result['siblings'])
>>> siblings
   age   name    pet
0   25  Scott   Zuko
1   33  Katie  Cisco


###XML和HTML:Web信息收集###
p185 有问题！！！！



###读取Microsoft Excel文件###
>>> xls_file=pd.ExcelFile('data.xlsx')		# 创建一个ExcelFile的实例
>>> xls_file
<pandas.io.excel.ExcelFile object at 0x0591DAF0>
>>> table=xls_file.parse('Sheet1')		# 通过parse读取到DataFrame中的数据，sheet表示表格形式1
>>> table
   1  2
0  3  4
1  5  6
2  7  8
>>> table=xls_file.parse()
>>> table
   1  2
0  3  4
1  5  6
2  7  8
>>> table=xls_file.parse('Sheet1', header=None)
>>> table
   0  1
0  1  2
1  3  4
2  5  6
3  7  8

>>> table=xls_file.parse('Sheet1', header=None)		# 如果没有会生成NaN
>>> table
     0  1    2
0  1.0  2  2.0
1  3.0  4  NaN
2  5.0  6  NaN
3  7.0  8  NaN
4  NaN  3  NaN
p192 这一章节没有好好看







##########################
#######数据规整化#########
#########################

###数据库风格的DataFrame合并###
>>> import pandas as pd
>>> from pandas import DataFrame
>>> df1=DataFrame({'key':['b', 'b', 'a', 'c', 'a', 'a', 'b'],
... 'data1':range(7)})
>>> df1
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   a
6      6   b
>>> df2=DataFrame({'key':['a', 'b', 'd'], 'data2':range(3)})
>>> df2
   data2 key
0      0   a
1      1   b
2      2   d

>>> df4=pd.merge(df1, df2)	# 多对一合并，如果没有指定，merge就会将重叠列的列名当做键
>>> df4				# 最好显示指定一下
   data1 key  data2
0      0   b      1
1      1   b      1
2      6   b      1
3      2   a      0
4      4   a      0
5      5   a      0

>>> pd.merge(df1, df2, on='key')
   data1 key  data2
0      0   b      1
1      1   b      1
2      6   b      1
3      2   a      0
4      4   a      0
5      5   a      0

>>> df4=DataFrame(df4, columns=['key', 'data1', 'data2'])	# 改变列的位置顺序
>>> df4
  key  data1  data2
0   b      0      1
1   b      1      1
2   b      6      1
3   a      2      0
4   a      4      0
5   a      5      0

>>> df4=DataFrame({'rkey':['a', 'b', 'd'], 'data2':range(3)})		# 两个对象的列名不同，也可以分别进行指定
>>> df3=DataFrame({'lkey':['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)})	# 先指定那个元素，就先列出那个元素
>>> pd.merge(df3, df4, left_on='lkey', right_on='rkey')
   data1 lkey  data2 rkey
0      0    b      1    b
1      1    b      1    b
2      6    b      1    b
3      2    a      0    a
4      4    a      0    a
5      5    a      0    a
>>> pd.merge(df4, df3, left_on='rkey', right_on='lkey')
   data2 rkey  data1 lkey
0      0    a      2    a
1      0    a      4    a
2      0    a      5    a
3      1    b      0    b
4      1    b      1    b
5      1    b      6    b

>>> pd.merge(df1, df2, how='outer')	# 外连接求取的是键的并集
   data1 key  data2
0    0.0   b    1.0
1    1.0   b    1.0
2    6.0   b    1.0
3    2.0   a    0.0
4    4.0   a    0.0
5    5.0   a    0.0
6    3.0   c    NaN
7    NaN   d    2.0

>>> pd.merge(df4, df3, left_on='rkey', right_on='lkey', how='outer')
   data2 rkey  data1 lkey
0    0.0    a    2.0    a
1    0.0    a    4.0    a
2    0.0    a    5.0    a
3    1.0    b    0.0    b
4    1.0    b    1.0    b
5    1.0    b    6.0    b
6    2.0    d    NaN  NaN
7    NaN  NaN    3.0    c

多对多的合并操作
>>> df1=DataFrame({'key':['b', 'b', 'a', 'c', 'a', 'b'],
... 'data1':range(6)})
>>> df2=DataFrame({'key':['a', 'b', 'a', 'b', 'd'], 'data2':range(5)})
>>> df1
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   b
>>> df2
   data2 key
0      0   a
1      1   b
2      2   a
3      3   b
4      4   d

>>> pd.merge(df1, df2, on='key', how='left')	# 每种组会都排列出来
    data1 key  data2
0       0   b    1.0
1       0   b    3.0
2       1   b    1.0
3       1   b    3.0
4       2   a    0.0
5       2   a    2.0
6       3   c    NaN
7       4   a    0.0
8       4   a    2.0
9       5   b    1.0
10      5   b    3.0

###根据多个建进行合并###
>>> left=DataFrame({'key1':['foo', 'foo', 'bar'],
... 'key2':['one', 'two', 'one'],
... 'lval':[1, 2, 3]})
>>> right=DataFrame({'key1':['foo', 'foo', 'bar', 'bar'],
... 'key2':['one', 'one', 'one', 'two'],
... 'rval':[4, 5, 6, 7]})
>>> left
  key1 key2  lval
0  foo  one     1
1  foo  two     2
2  bar  one     3
>>> right
  key1 key2  rval
0  foo  one     4
1  foo  one     5
2  bar  one     6
3  bar  two     7
>>> pd.merge(left, right, on=['key1', 'key2'], how='outer')
  key1 key2  lval  rval
0  foo  one   1.0   4.0
1  foo  one   1.0   5.0
2  foo  two   2.0   NaN
3  bar  one   3.0   6.0
4  bar  two   NaN   7.0
>>> pd.merge(left, right, on=['key1'])
  key1 key2_x  lval key2_y  rval
0  foo    one     1    one     4
1  foo    one     1    one     5
2  foo    two     2    one     4
3  foo    two     2    one     5
4  bar    one     3    one     6
5  bar    one     3    two     7

>>> pd.merge(left, right, on=['key1'], suffixes=('_left', '_right'))	# 列名重叠后缀
  key1 key2_left  lval key2_right  rval
0  foo       one     1        one     4
1  foo       one     1        one     5
2  foo       two     2        one     4
3  foo       two     2        one     5
4  bar       one     3        one     6
5  bar       one     3        two     7


###索引上的合并###
>>> left1=DataFrame({'key':['a', 'b', 'a', 'a', 'b', 'c'], 'value':range(6)})
>>> right1=DataFrame({'group_val':[3.5, 7]}, index=['a', 'b'])
>>> left1
  key  value
0   a      0
1   b      1
2   a      2
3   a      3
4   b      4
5   c      5
>>> right1
   group_val
a        3.5
b        7.0
>>> pd.merge(left1, right1, left_on='key', right_index=True)
  key  value  group_val
0   a      0        3.5
2   a      2        3.5
3   a      3        3.5
1   b      1        7.0
4   b      4        7.0
>>> pd.merge(left1, right1, left_on='key', right_index=True, how='outer')	# 得到并集
  key  value  group_val
0   a      0        3.5
2   a      2        3.5
3   a      3        3.5
1   b      1        7.0
4   b      4        7.0
5   c      5        NaN

###层次化索引的数据###
>>> left=DataFrame({'key1':['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
... 'key2':[2000, 2001, 2002, 2001, 2002],
... 'data':np.arange(5.)})
>>> righth=DataFrame(np.arange(12).reshape((6, 2)),
... index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'],
... [2001, 2000, 2000, 2000, 2001, 2002]],
... columns=['event1', 'event2'])
>>> left
   data    key1  key2
0   0.0    Ohio  2000
1   1.0    Ohio  2001
2   2.0    Ohio  2002
3   3.0  Nevada  2001
4   4.0  Nevada  2002
>>> righth
             event1  event2
Nevada 2001       0       1
       2000       2       3
Ohio   2000       4       5
       2000       6       7
       2001       8       9
       2002      10      11
>>> pd.merge(left, righth, left_on=['key1', 'key2'], right_index=True)
   data    key1  key2  event1  event2
0   0.0    Ohio  2000       4       5
0   0.0    Ohio  2000       6       7
1   1.0    Ohio  2001       8       9
2   2.0    Ohio  2002      10      11
3   3.0  Nevada  2001       0       1

>>> pd.merge(left, righth, left_on=['key1', 'key2'], right_index=True, how='outer')
   data    key1    key2  event1  event2
0   0.0    Ohio  2000.0     4.0     5.0
0   0.0    Ohio  2000.0     6.0     7.0
1   1.0    Ohio  2001.0     8.0     9.0
2   2.0    Ohio  2002.0    10.0    11.0
3   3.0  Nevada  2001.0     0.0     1.0
4   4.0  Nevada  2002.0     NaN     NaN
4   NaN  Nevada  2000.0     2.0     3.0

###合并双方的索引###
>>> left2=DataFrame([[1, 2], [3, 4],[5, 6]], index=['a', 'c', 'e'],
... columns=['Ohio', 'Nevada'])
>>> right2=DataFrame([[7, 8], [9, 10], [11, 12],  [13, 14]],
... index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])
>>> left2
   Ohio  Nevada
a     1       2
c     3       4
e     5       6
>>> right2
   Missouri  Alabama
b         7        8
c         9       10
d        11       12
e        13       14
>>> pd.merge(left2, right2, how='outer', left_index=True, right_index=True)
   Ohio  Nevada  Missouri  Alabama
a   1.0     2.0       NaN      NaN
b   NaN     NaN       7.0      8.0
c   3.0     4.0       9.0     10.0
d   NaN     NaN      11.0     12.0
e   5.0     6.0      13.0     14.0

>>> left2.join(right2, how='outer')		# join实例方法
   Ohio  Nevada  Missouri  Alabama
a   1.0     2.0       NaN      NaN
b   NaN     NaN       7.0      8.0
c   3.0     4.0       9.0     10.0
d   NaN     NaN      11.0     12.0
e   5.0     6.0      13.0     14.0

>>> left1.join(right1, on='key')		# join支持DataFrame的某个列之间的链接
  key  value  group_val
0   a      0        3.5
1   b      1        7.0
2   a      2        3.5
3   a      3        3.5
4   b      4        7.0
5   c      5        NaN

>>> another=DataFrame([[7., 8.], [9. ,10.], [11., 12.], [16. ,17.]],
... index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])
>>> another
   New York  Oregon
a       7.0     8.0
c       9.0    10.0
e      11.0    12.0
f      16.0    17.0
>>> left2.join([right2, another])		# 还可以向join传入一组DataFrame
   Ohio  Nevada  Missouri  Alabama  New York  Oregon
a     1       2       NaN      NaN       7.0     8.0
c     3       4       9.0     10.0       9.0    10.0
e     5       6      13.0     14.0      11.0    12.0


###轴向连接###
>>> arr=np.array(np.arange(12).reshape(3,4))
>>> arr
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])
>>> np.concatenate([arr, arr], axis=1)		# 行连接 Numpy合并
array([[ 0,  1,  2,  3,  0,  1,  2,  3],
       [ 4,  5,  6,  7,  4,  5,  6,  7],
       [ 8,  9, 10, 11,  8,  9, 10, 11]])
>>> np.concatenate([arr, arr], axis=0)		# 列连接
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])

###DataFrame连接###
>>> s1=Series([0, 1], index=['a', 'b'])
>>> s2=Series([2, 3, 4], index=['c', 'd', 'e'])
>>> s3=Series([5, 6], index=['f', 'g'])
>>> pd.concat([s1, s2, s3])
a    0
b    1
c    2
d    3
e    4
f    5
g    6
dtype: int64
>>> s1
a    0
b    1
dtype: int64
>>> s2
c    2
d    3
e    4
dtype: int64
>>> s3
f    5
g    6
dtype: int64
>>> pd.concat([s1, s2, s3], axis=1)
     0    1    2
a  0.0  NaN  NaN
b  1.0  NaN  NaN
c  NaN  2.0  NaN
d  NaN  3.0  NaN
e  NaN  4.0  NaN
f  NaN  NaN  5.0
g  NaN  NaN  6.0

>>> s4=pd.concat([s1*5, s3])
>>> s4
a    0
b    5
f    5
g    6

>>> pd.concat([s1, s4], axis=1, join='inner')	# join='inner'即可得到它们的交集
   0  1
a  0  0
b  1  5

>>> pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'f']])	# 通过join_axes指定要在其他轴上使用的索引
     0    1
a  0.0  0.0
c  NaN  NaN
b  1.0  5.0
f  NaN  5.0

>>> result=pd.concat([s1, s1, s3], keys=['one', 'two' ,'three'])
>>> result
one    a    0
       b    1
two    a    0
       b    1
three  f    5
       g    6
dtype: int64


>>> result=pd.concat([s1, s1, s3], keys=['one', 'two' ,'three'])	# 假设你想要在连接轴上创建一个层次化
>>> result								# 将参与连接的片段在结果中区分开
one    a    0
       b    1
two    a    0
       b    1
three  f    5
       g    6
dtype: int64
>>> result.unstack()		# unstack函数
         a    b    f    g
one    0.0  1.0  NaN  NaN
two    0.0  1.0  NaN  NaN
three  NaN  NaN  5.0  6.0

>>> pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])	# 沿着axis=1对Series进行合并
   one  two  three							# 则keys就会成为DataFrame的列头
a  0.0  NaN    NaN
b  1.0  NaN    NaN
c  NaN  2.0    NaN
d  NaN  3.0    NaN
e  NaN  4.0    NaN
f  NaN  NaN    5.0
g  NaN  NaN    6.0


>>> df1=DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'],
... columns=['one', 'two'])
>>> df2=DataFrame(5+np.arange(4).reshape(2,2), index=['a','c'],
... columns=['three', 'four'])
>>> df1
   one  two
a    0    1
b    2    3
c    4    5
>>> df2
   three  four
a      5     6
c      7     8
>>> pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])	#
  level1     level2
     one two  three four
a      0   1    5.0  6.0
b      2   3    NaN  NaN
c      4   5    7.0  8.0

>>> pd.concat({'level1':df1, 'level2':df2}, axis=1)		# 传入的不是列表而是一个字典
  level1     level2						# 则字典的键就会被当做keys选项的值
     one two  three four
a      0   1    5.0  6.0
b      2   3    NaN  NaN
c      4   5    7.0  8.0

>>> pd.concat([df1, df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower'])	# 管理层次化索引创建方式的参数
upper level1     level2
lower    one two  three four
a          0   1    5.0  6.0
b          2   3    NaN  NaN
c          4   5    7.0  8.0


>>> df1=DataFrame(np.random.randn(3,4), columns=['a', 'b', 'c', 'd'])
>>> df2=DataFrame(np.random.randn(2,3), columns=['b', 'd', 'a'])
>>> df1
          a         b         c         d
0  0.425610 -0.301489  1.098388  1.082600
1 -0.526522  0.443978 -0.620161 -0.432289
2  0.287441 -0.479613  0.214522  0.036117
>>> df2
          b         d         a
0  0.535586  1.977750  1.278601
1 -0.508055 -0.111801  0.086542
>>> pd.concat([df1, df2])
          a         b         c         d
0  0.425610 -0.301489  1.098388  1.082600
1 -0.526522  0.443978 -0.620161 -0.432289
2  0.287441 -0.479613  0.214522  0.036117
0  1.278601  0.535586       NaN  1.977750
1  0.086542 -0.508055       NaN -0.111801
>>> pd.concat([df1, df2], ignore_index=True)	# 处理与当前工作无关的DataFrame行索引
          a         b         c         d
0  0.425610 -0.301489  1.098388  1.082600
1 -0.526522  0.443978 -0.620161 -0.432289
2  0.287441 -0.479613  0.214522  0.036117
3  1.278601  0.535586       NaN  1.977750
4  0.086542 -0.508055       NaN -0.111801

###合并重叠数据###

>>> a=Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan], index=['f', 'e', 'd', 'c', 'b', 'a'])
>>> a
f    NaN
e    2.5
d    NaN
c    3.5
b    4.5
a    NaN
dtype: float64

>>> b=Series(np.arange(len(a), dtype=np.float64), index=['f', 'e', 'd', 'c', 'b', 'a'])
>>> b
f    0.0
e    1.0
d    2.0
c    3.0
b    4.0
a    5.0
dtype: float64

>>> b[-1]=np.nan
>>> b
f    0.0
e    1.0
d    2.0
c    3.0
b    4.0
a    NaN
dtype: float64

>>> np.where(pd.isnull(a), b, a)	# 如果pd.isnull()是nan 则输出b中的数，否则输出a中的数
array([ 0. ,  2.5,  2. ,  3.5,  4.5,  nan])

>>> b[:-2].combine_first(a[2:])		# b[:-2]中没有，则用a[2:]中的数值
a    NaN
b    4.5
c    3.0
d    2.0
e    1.0
f    0.0
dtype: float64

>>> a.combine_first(b)
f    0.0
e    2.5
d    2.0
c    3.5
b    4.5
a    NaN
dtype: float64

>>> df1=DataFrame({'a':[1, np.nan, 5., np.nan],
... 'b':[np.nan, 2., np.nan, 6.],
... 'c':range(2, 18, 4)})
>>> df2=DataFrame({'a':[5., 4., np.nan, 3., 7.],
... 'b':[np.nan, 3., 4., 6., 8.]})
>>> df1
     a    b   c
0  1.0  NaN   2
1  NaN  2.0   6
2  5.0  NaN  10
3  NaN  6.0  14
>>> df2
     a    b
0  5.0  NaN
1  4.0  3.0
2  NaN  4.0
3  3.0  6.0
4  7.0  8.0
>>> df1.combine_first(df2)	# 用参数对象中的数据为调用者对象的缺失数据“打补丁”
     a    b     c
0  1.0  NaN   2.0
1  4.0  2.0   6.0
2  5.0  4.0  10.0
3  3.0  6.0  14.0
4  7.0  8.0   NaN

###重塑和轴向旋转###

###重塑层次化索引###
>>> from pandas import DataFrame
>>> data=DataFrame(np.arange(6).reshape((2,3)), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['
ne', 'two', 'three'], name='number'))
>>> data
number    one  two  three
state
Ohio        0    1      2
Colorado    3    4      5
>>> result=data.stack()		# 使用该数据的stack方法即可将列转换为行，得到一个Series
>>> result
state     number
Ohio      one       0
          two       1
          three     2
Colorado  one       3
          two       4
          three     5
dtype: int32
>>> result.unstack()
number    one  two  three	# 对于一个层次化索引的Series
state				# 可以用unstack将其重排为一个DataFrame
Ohio        0    1      2
Colorado    3    4      5

>>> result.unstack(1)		# 默认情况下，unstack操作的是最内层
number    one  two  three	# unstack（1）操作最内层
state
Ohio        0    1      2
Colorado    3    4      5
>>> result.unstack(0)		# 外层
state   Ohio  Colorado
number
one        0         3
two        1         4
three      2         5

>>> s1=Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])
>>> s2=Series([4, 5, 6], index=['c', 'd', 'e'])
>>> data2=pd.concat([s1, s2], keys=['one', 'two'])
>>> data2
one  a    0
     b    1
     c    2
     d    3
two  c    4
     d    5
     e    6
dtype: int64
>>> data2.unstack()	# unstack操作可能会引入缺失数据
       a    b    c    d    e
one  0.0  1.0  2.0  3.0  NaN
two  NaN  NaN  4.0  5.0  6.0

>>> data2.unstack().stack()	#stack默认会滤除缺失数据，因此该运算时可逆的
one  a    0.0
     b    1.0
     c    2.0
     d    3.0
two  c    4.0
     d    5.0
     e    6.0
dtype: float64

>>> data2.unstack().stack(dropna=False)
one  a    0.0
     b    1.0
     c    2.0
     d    3.0
     e    NaN
two  a    NaN
     b    NaN
     c    4.0
     d    5.0
     e    6.0
dtype: float64

>>> df=DataFrame({'left':result, 'right':result+5}, columns=pd.Index(['left', 'right'], name='side'))
>>> df
side             left  right
state    number
Ohio     one        0      5
         two        1      6
         three      2      7
Colorado one        3      8
         two        4      9
         three      5     10
>>> df.unstack('state')		# 旋转轴的级别将会是结果中的最低级别
side   left          right
state  Ohio Colorado  Ohio Colorado
number
one       0        3     5        8
two       1        4     6        9
three     2        5     7       10

>>> df.unstack('state').stack('side')
state         Ohio  Colorado
number side
one    left      0         3
       right     5         8
two    left      1         4
       right     6         9
three  left      2         5
       right     7        10


###将“长格式”旋转为“宽格式”###
>>> import pandas as pd
>>> from datetime import *
>>> from pandas import DataFrame
>>> data=pd.read_csv('macrodata.csv')		# 读取csv文件
>>> temp_data=DataFrame(data, columns=['year', 'quarter', 'realgdp', 'unemp'])	# 从data中读取相应行
>>> temp_data[:2]
     year  quarter   realgdp  unemp
0  1959.0      1.0  2710.349    5.8
1  1959.0      2.0  2778.801    5.1

>>> for i in range(len(temp_data.index)):
...     if int(temp_data[i:i+1].quarter) == 1:
...             temp_data.loc[i, 'date']=datetime(temp_data[i:i+1].year, 3, 31)
...     elif int(temp_data[i:i+1].quarter) == 2:
...             temp_data.loc[i, 'date']=datetime(temp_data[i:i+1].year, 6, 30)
...     elif int(temp_data[i:i+1].quarter) == 3:
...             temp_data.loc[i, 'date']=datetime(temp_data[i:i+1].year, 9, 30)
...     elif int(temp_data[i:i+1].quarter) == 4:
...             temp_data.loc[i, 'date']=datetime(temp_data[i:i+1].year, 12, 31)
...
>>> temp_data[0:4]
     year  quarter   realgdp  unemp                 date
0  1959.0      1.0  2710.349    5.8  1959-03-31 00:00:00
1  1959.0      2.0  2778.801    5.1  1959-06-30 00:00:00
2  1959.0      3.0  2775.488    5.3  1959-09-30 00:00:00
3  1959.0      4.0  2785.204    5.6  1959-12-31 00:00:00

>>> temp_data=temp_data.drop(['year', 'quarter'], 1).set_index('date').stack()
>>> temp_data[0:4]
date
1959-03-31  realgdp    2710.349
            unemp         5.800
1959-06-30  realgdp    2778.801
            unemp         5.100
dtype: float64

###文本转换程序###
PS C:\Users\Weihong> python generateTable.py
     year  quarter   realgdp  infl  unemp       date
0  1959.0      1.0  2710.349  0.00    5.8 1959-03-31
1  1959.0      2.0  2778.801  2.34    5.1 1959-06-30
2  1959.0      3.0  2775.488  2.74    5.3 1959-09-30
3  1959.0      4.0  2785.204  0.27    5.6 1959-12-31
         date     item     value
0  1959-03-31  realgdp  2710.349
1  1959-03-31     infl     0.000
2  1959-03-31    unemp     5.800
3  1959-06-30  realgdp  2778.801
4  1959-06-30     infl     2.340
5  1959-06-30    unemp     5.100
6  1959-09-30  realgdp  2775.488
7  1959-09-30     infl     2.740
8  1959-09-30    unemp     5.300
9  1959-12-31  realgdp  2785.204
PS C:\Users\Weihong> cat .\generateTable.py
import pandas as pd
from pandas import DataFrame
from datetime import *

df=pd.read_csv('macrodata.csv')
temp_data=DataFrame(df, columns=['year', 'quarter', 'realgdp', 'infl', 'unemp'])

for i in range(len(temp_data.index)):
        if int(temp_data.loc[i, 'quarter'])==1:
                temp_data.loc[i, 'date']=datetime(int(temp_data[i:i+1].year), 3, 31)
        if int(temp_data.loc[i, 'quarter'])==2:
                temp_data.loc[i, 'date']=datetime(int(temp_data[i:i+1].year), 6, 30)
        if int(temp_data.loc[i, 'quarter'])==3:
                temp_data.loc[i, 'date']=datetime(int(temp_data[i:i+1].year), 9, 30)
        if int(temp_data.loc[i, 'quarter'])==4:
                temp_data.loc[i, 'date']=datetime(int(temp_data[i:i+1].year), 12, 31)

print temp_data[:4]
temp_data=temp_data.drop(['year', 'quarter'], axis=1).set_index('date').stack()

temp_data.to_csv('new.csv')
new_df=pd.read_csv('new.csv', names=['date', 'item', 'value'])
print new_df[:10]


>>> ldata=pd.read_csv('new.csv', names=['date', 'item', 'value'])
>>> ldata[:10]
         date     item     value
0  1959-03-31  realgdp  2710.349
1  1959-03-31     infl     0.000
2  1959-03-31    unemp     5.800
3  1959-06-30  realgdp  2778.801
4  1959-06-30     infl     2.340
5  1959-06-30    unemp     5.100
6  1959-09-30  realgdp  2775.488
7  1959-09-30     infl     2.740
8  1959-09-30    unemp     5.300
9  1959-12-31  realgdp  2785.204
>>> pivoted=ldata.pivot('date', 'item', 'value')	# 前两个参数值分别用作行和列索引的列名，最后一个参数则是
>>> pivoted.head()					# 填充DataFrame的数据列的列名
item        infl   realgdp  unemp
date
1959-03-31  0.00  2710.349    5.8
1959-06-30  2.34  2778.801    5.1
1959-09-30  2.74  2775.488    5.3
1959-12-31  0.27  2785.204    5.6
1960-03-31  2.31  2847.699    5.2

>>> ldata['value2']=np.random.randn(len(ldata))		# 增加一列
>>> ldata[:10]
         date     item     value    value2
0  1959-03-31  realgdp  2710.349 -0.538073
1  1959-03-31     infl     0.000 -0.959795
2  1959-03-31    unemp     5.800 -1.380172
3  1959-06-30  realgdp  2778.801 -0.731581
4  1959-06-30     infl     2.340  1.231449
5  1959-06-30    unemp     5.100  1.240695
6  1959-09-30  realgdp  2775.488 -0.797257
7  1959-09-30     infl     2.740  0.880758
8  1959-09-30    unemp     5.300 -1.134288
9  1959-12-31  realgdp  2785.204 -0.310144

>>> pivoted=ldata.pivot('date', 'item')		# 忽略最后一个参数，得到DataFrame就会带有层次化的列
>>> pivoted[:5]		
           value                    value2
item        infl   realgdp unemp      infl   realgdp     unemp
date
1959-03-31  0.00  2710.349   5.8 -0.959795 -0.538073 -1.380172
1959-06-30  2.34  2778.801   5.1  1.231449 -0.731581  1.240695
1959-09-30  2.74  2775.488   5.3  0.880758 -0.797257 -1.134288
1959-12-31  0.27  2785.204   5.6  0.031874 -0.310144 -1.437487
1960-03-31  2.31  2847.699   5.2 -2.354331 -0.104462  2.100655

>>> pivoted['value'][:5]
item        infl   realgdp  unemp
date
1959-03-31  0.00  2710.349    5.8
1959-06-30  2.34  2778.801    5.1
1959-09-30  2.74  2775.488    5.3
1959-12-31  0.27  2785.204    5.6
1960-03-31  2.31  2847.699    5.2

>>> unstacked=ldata.set_index(['date', 'item']).unstack('item')		# 用set_index创建层次化索引，再用unstack重塑
>>> unstacked[:7]
           value                    value2
item        infl   realgdp unemp      infl   realgdp     unemp
date
1959-03-31  0.00  2710.349   5.8 -0.959795 -0.538073 -1.380172
1959-06-30  2.34  2778.801   5.1  1.231449 -0.731581  1.240695
1959-09-30  2.74  2775.488   5.3  0.880758 -0.797257 -1.134288
1959-12-31  0.27  2785.204   5.6  0.031874 -0.310144 -1.437487
1960-03-31  2.31  2847.699   5.2 -2.354331 -0.104462  2.100655
1960-06-30  0.14  2834.390   5.2  0.629474 -1.149575 -1.441395
1960-09-30  2.70  2839.022   5.6 -0.208390 -0.751707  1.276936



############
##数据转换##
############

###移除重复数据###
>>> data=DataFrame({'k1':['one']*3+['two']*4, 'k2':[1, 1, 2, 3, 3, 4, 4]})
>>> data
    k1  k2
0  one   1
1  one   1
2  one   2
3  two   3
4  two   3
5  two   4
6  two   4
>>> data.duplicated()	# 返回一个布尔型Series，表示各行是否重复
0    False
1     True
2    False
3    False
4     True
5    False
6     True
dtype: bool

>>> data.drop_duplicates()	# 返回一个自出了重复行的DataFrame
    k1  k2
0  one   1
2  one   2
3  two   3
5  two   4

>>> data['v1']=range(7)
>>> data
    k1  k2  v1
0  one   1   0
1  one   1   1
2  one   2   2
3  two   3   3
4  two   3   4
5  two   4   5
6  two   4   6
>>> data.drop_duplicates(['k1'])	# 删除k1列中重复的行
    k1  k2  v1
0  one   1   0
3  two   3   3
>>> data.drop_duplicates(['k1', 'k2'])
    k1  k2  v1
0  one   1   0
2  one   2   2
3  two   3   3
5  two   4   5
>>> data.drop_duplicates(['k1', 'k2'], keep='last')	# 传入keep=‘last’则保留最后一个
    k1  k2  v1
1  one   1   1
2  one   2   2
4  two   3   4
6  two   4   6


###利用函数或映射进行数据转换###
>>> data
          food  ounces
0        bacon     4.0
1  pulled pork     3.0
2        bacon    12.0
3     Pastrami     6.0
4  corned beef     7.5
5        Bacon     8.0
6     pastrami     3.0
7    honey ham     5.0
8     nova lox     6.0

>>> meat_to_animal
{'pastrami': 'cow', 'bacon': 'pig', 'pulled pork': 'pig', 'corned beef': 'cow', 'nova lox': 'salmon', 'honey ham': 'pig'
}
>>> data['animal']=data['food'].map(str.lower).map(meat_to_animal)	# map()也是element-wise的，对Series中的每个数据调用一次函数。
>>> data
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     Pastrami     6.0     cow
4  corned beef     7.5     cow
5        Bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon
>>>

>>> data['food'].map(lambda x: meat_to_animal[x.lower()])	# 也可以传入一个能够完成全部这些工作的函数
0       pig
1       pig
2       pig
3       cow
4       cow
5       pig
6       cow
7       pig
8    salmon
Name: food, dtype: object
>>> data
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     Pastrami     6.0     cow
4  corned beef     7.5     cow
5        Bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon


###替换值###
>>> data=Series([1, -999, 2, -999, -1000, 3])
>>> data
0       1
1    -999
2       2
3    -999
4   -1000
5       3
dtype: int64
>>> data.replace(-999, np.nan)	# replace产生一个新的Series
0       1.0
1       NaN
2       2.0
3       NaN
4   -1000.0
5       3.0
dtype: float64
>>> data
0       1
1    -999
2       2
3    -999
4   -1000
5       3
dtype: int64
>>> data.replace([-999, -1000], np.nan)		# 一次性替换多个值
0    1.0
1    NaN
2    2.0
3    NaN
4    NaN
5    3.0
dtype: float64

>>> data.replace([-999, -1000],[np.nan, 0])	# 关系列表
0    1.0
1    NaN
2    2.0
3    NaN
4    0.0
5    3.0
dtype: float64

>>> data.replace({-999:np.nan, -1000: 0})	# 传入参数可以是字典
0    1.0
1    NaN
2    2.0
3    NaN
4    0.0
5    3.0
dtype: float64
p219

###重命名轴索引###
>>> import numpy as np
>>> import pandas as pd
>>> from pandas import Series
>>> from pandas import DataFrame
>>> data=DataFrame(np.arange(12).reshape((3,4)), index=['Ohio', 'Colorado', 'New York'], columns=['one', 'two', 'three',
 'four'])
>>> data
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11
>>> data.index.map(str.upper)
array(['OHIO', 'COLORADO', 'NEW YORK'], dtype=object)
>>> data
          one  two  three  four
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11
>>> data.index=data.index.map(str.upper)	# 将新值赋值给index
>>> data
          one  two  three  four
OHIO        0    1      2     3
COLORADO    4    5      6     7
NEW YORK    8    9     10    11
>>> data.rename(index=str.title, columns=str.upper)	# 创建数据集的转换版（而不是修改原始数据），比较实用的方法是rename。
          ONE  TWO  THREE  FOUR
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11
>>> data		# data没改变
          one  two  three  four
OHIO        0    1      2     3
COLORADO    4    5      6     7
NEW YORK    8    9     10    11
>>> data.rename(index={'OHIO':'INDIANA'}, columns={'three':'peekaboo'})		# rename可以结合字典
          one  two  peekaboo  four						# 型对象实现对部分轴标签的更新与赋值
INDIANA     0    1         2     3
COLORADO    4    5         6     7
NEW YORK    8    9        10    11
>>> data.rename(index={'OHIO':'INDIANA'}, inplace=True)				# 加入inplace=True即可就地修改某个数据
>>> data
          one  two  three  four
INDIANA     0    1      2     3
COLORADO    4    5      6     7
NEW YORK    8    9     10    11

###离散化和面元划分###
>>> ages=[20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
>>> bins=[18, 25, 35, 60, 100]
>>> cats=pd.cut(ages, bins)	# 实用pandas的cut函数将数据划分为几个category
>>> cats
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]	# 每个数属于哪个category
Length: 12
Categories (4, object): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]
>>> cats.labels
__main__:1: FutureWarning: 'labels' is deprecated. Use 'codes' instead
array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)
>>> cats.levels
__main__:1: FutureWarning: Accessing 'levels' is deprecated, use 'categories'
Index([u'(18, 25]', u'(25, 35]', u'(35, 60]', u'(60, 100]'], dtype='object')
>>> pd.value_counts(cats)
(18, 25]     5
(35, 60]     3
(25, 35]     3
(60, 100]    1
dtype: int64

>>> pd.cut(ages, [18, 26, 36, 61, 100], right=False)		# 圆括号表示开端，而方括号则表示闭端
[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)]
Length: 12
Categories (4, object): [[18, 26) < [26, 36) < [36, 61) < [61, 100)]


>>> group_names=['Youth', 'YoungAdult', 'MiddleAged', 'Senior']		# 可以设置自己的面元名称
>>> cat2=pd.cut(ages, bins, labels=group_names)
>>> pd.value_counts(cat2)
Youth         5
MiddleAged    3
YoungAdult    3
Senior        1
dtype: int64


>>> data=np.random.rand(20)
>>> data
array([ 0.16893799,  0.38738474,  0.08677298,  0.63193194,  0.75287526,
        0.03716956,  0.62824345,  0.71150753,  0.43251804,  0.70089543,
        0.86703585,  0.68628439,  0.21686957,  0.06652676,  0.63972259,
        0.06435226,  0.75324056,  0.75501045,  0.25249319,  0.60600454])
>>> pd.cut(data, 4, precision=2)	# 根据数据的最小值和最大值计算 等长面元
[(0.036, 0.24], (0.24, 0.45], (0.036, 0.24], (0.45, 0.66], (0.66, 0.87], ..., (0.036, 0.24], (0.66, 0.87], (0.66, 0.87],
 (0.24, 0.45], (0.45, 0.66]]
Length: 20
Categories (4, object): [(0.036, 0.24] < (0.24, 0.45] < (0.45, 0.66] < (0.66, 0.87]]
>>> cat=pd.cut(data, 4, precision=2)
>>> pd.value_counts(cat)
(0.66, 0.87]     7
(0.036, 0.24]    6
(0.45, 0.66]     4
(0.24, 0.45]     3
dtype: int64

>>> data=np.random.randn(1000)	# 正态分布
>>> cats=pd.qcut(data, 4)	# qcut可以得到大小基本相等的面元
>>> cats
[[-2.942, -0.722], (-0.0636, 0.634], [-2.942, -0.722], (0.634, 3.282], [-2.942, -0.722], ..., (-0.0
, 0.634], (-0.722, -0.0636], (-0.722, -0.0636], (-0.722, -0.0636]]
Length: 1000
Categories (4, object): [[-2.942, -0.722] < (-0.722, -0.0636] < (-0.0636, 0.634] < (0.634, 3.282]]
>>> pd.value_counts(cats)
(0.634, 3.282]       250
(-0.0636, 0.634]     250
(-0.722, -0.0636]    250
[-2.942, -0.722]     250


>>> pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])
[[-2.942, -1.356], (-0.0636, 1.262], (-1.356, -0.0636], (-0.0636, 1.262], [-2.942, -1.356], ..., (-0.0636,
636, 1.262], (-1.356, -0.0636], (-1.356, -0.0636], (-1.356, -0.0636]]
Length: 1000
Categories (4, object): [[-2.942, -1.356] < (-1.356, -0.0636] < (-0.0636, 1.262] < (1.262, 3.282]]
>>> cat=pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])		# 跟cut一样，可以设置自定义的分位数 从前10% 40% 40% 10%
>>> pd.value_counts(cat)
(-0.0636, 1.262]     400
(-1.356, -0.0636]    400
(1.262, 3.282]       100
[-2.942, -1.356]     100
dtype: int64


###检测和过滤异常值###

>>> np.random.seed(12345)
>>> data=DataFrame(np.random.randn(1000, 4))
>>> data.describe()
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean     -0.067684     0.067924     0.025598    -0.002298
std       0.998035     0.992106     1.006835     0.996794
min      -3.428254    -3.548824    -3.184377    -3.745356
25%      -0.774890    -0.591841    -0.641675    -0.644144
50%      -0.116401     0.101143     0.002073    -0.013611
75%       0.616366     0.780282     0.680391     0.654328
max       3.366626     2.653656     3.260383     3.927528
>>> col=data[3]
>>> col[np.abs(col)>3]
97     3.927528
305   -3.399312
400   -3.745356
Name: 3, dtype: float64

>>> data[(np.abs(data)>3).any(1)]	# data中 任意行包含绝对值大于3的行
            0         1         2         3
5   -0.539741  0.476985  3.248944 -1.021228
97  -0.774363  0.552936  0.106061  3.927528
102 -0.655054 -0.565230  3.176873  0.959533
305 -2.315555  0.457246 -0.025907 -3.399312
324  0.050188  1.951312  3.260383  0.963301
400  0.146326  0.508391 -0.196713 -3.745356
499 -0.293333 -0.242459 -3.056990  1.918403
523 -3.428254 -0.296336 -0.439938 -0.867165
586  0.275144  1.179227 -3.184377  1.369891
808 -0.362528 -3.548824  1.553205 -2.186301
900  3.366626 -2.372214  0.851010  1.332846


>>> data[np.abs(data)>3]=np.sign(data)*3	# np.sign返回的是一个由1和-1组成的数组
>>> data.describe()
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean     -0.067623     0.068473     0.025153    -0.002081
std       0.995485     0.990253     1.003977     0.989736
min      -3.000000    -3.000000    -3.000000    -3.000000
25%      -0.774890    -0.591841    -0.641675    -0.644144
50%      -0.116401     0.101143     0.002073    -0.013611
75%       0.616366     0.780282     0.680391     0.654328
max       3.000000     2.653656     3.000000     3.000000

###排列和随机采样###

>>> from pandas import DataFrame
>>> df=DataFrame(np.arange(5*4).reshape(5,4))
>>> sampler=np.random.permutation(5)	# 生成一个随机排列
>>> sampler
array([1, 0, 3, 2, 4])
>>> df
    0   1   2   3
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19
>>> df.take(sampler)			# 基于ix的索引操作或take函数中使用该数组
    0   1   2   3
1   4   5   6   7
0   0   1   2   3
3  12  13  14  15
2   8   9  10  11
4  16  17  18  19
>>> df1=DataFrame(np.arange(6*4).reshape(6,4))		
>>> df1
    0   1   2   3
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19
5  20  21  22  23
>>> df1.take(sampler)			# 由于sampler长度为5，只选取前5个
    0   1   2   3
1   4   5   6   7
0   0   1   2   3
3  12  13  14  15
2   8   9  10  11
4  16  17  18  19

>>> len(df)
5
>>> df.take(np.random.permutation(len(df))[:3])		# 从permutation返回的数组中切下前k个元素
    0   1   2   3
4  16  17  18  19
2   8   9  10  11
0   0   1   2   3

>>> bag=np.array([5, 7, -1, 6, 4])
>>> sampler=np.random.randint(0, len(bag), size=10)
>>> sampler
array([2, 1, 4, 3, 1, 0, 0, 1, 3, 4])
>>> draws=bag.take(sampler)		# 通过替换的方式产生样本
>>> draws
array([-1,  7,  4,  6,  7,  5,  5,  7,  6,  4])


###计算指标/哑变量###
>>> df=DataFrame({'key':['b', 'b', 'a', 'c', 'a', 'b'],
... 'data1':range(6)})
>>> df
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   b
>>> pd.get_dummies(df['key'])	# 将分类变量转换为“哑变量矩阵”或“指标矩阵”
     a    b    c		# 派生出一个k列矩阵或DataFrame（其值全为1和0）
0  0.0  1.0  0.0		# 自己可以写个程序
1  0.0  1.0  0.0
2  1.0  0.0  0.0
3  0.0  0.0  1.0
4  1.0  0.0  0.0
5  0.0  1.0  0.0

PS C:\Users\Weihong> cat .\dummy_wwh.py		# 自己实现生成哑变矩阵
import pandas as pd
import numpy as np
from pandas import Series
from pandas import DataFrame

df=DataFrame({'key':['b', 'b', 'a', 'c', 'a', 'b'],'data1':range(6)})
print df

new_list=[]
for i in range(len(df)):
        if df.loc[i, 'key']=='a':
                new_list.append([1, 0, 0])
        elif df.loc[i, 'key']=='b':
                new_list.append([0, 1, 0])
        elif df.loc[i, 'key']=='c':
                new_list.append([0, 0, 1])

new_df=DataFrame(new_list, columns=['a', 'b', 'c'])
print new_df

PS C:\Users\Weihong> python .\dummy_wwh.py	#
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   b
   a  b  c
0  0  1  0
1  0  1  0
2  1  0  0
3  0  0  1
4  1  0  0
5  0  1  0

>>> dummies=pd.get_dummies(df['key'], prefix='key')	# 可以给DataFrame的列加上一个前缀
>>> dummies
   key_a  key_b  key_c
0    0.0    1.0    0.0
1    0.0    1.0    0.0
2    1.0    0.0    0.0
3    0.0    0.0    1.0
4    1.0    0.0    0.0
5    0.0    1.0    0.0
>>> df_with_dummy=df[['data1']].join(dummies)		# df中的data1与dummies合并
>>> df_with_dummy
   data1  key_a  key_b  key_c
0      0    0.0    1.0    0.0
1      1    0.0    1.0    0.0
2      2    1.0    0.0    0.0
3      3    0.0    0.0    1.0
4      4    1.0    0.0    0.0
5      5    0.0    1.0    0.0


>>> mnames=['movie_id', 'title', 'genres']
>>> movies=pd.read_table('movies.dat', sep='::',header=None,  names=mnames)	# 读取文件， 分隔符为::
>>> movies[:3]
   movie_id                    title                        genres
0         1         Toy Story (1995)   Animation|Children's|Comedy
1         2           Jumanji (1995)  Adventure|Children's|Fantasy
2         3  Grumpier Old Men (1995)                Comedy|Romance
>>> genre_iter=(set(x.split('|')) for x in movies.genres)		# 生成一个迭代生成器
>>> genre_iter								# http://www.open-open.com/lib/view/open1463668934647.html
<generator object <genexpr> at 0x05816058>
>>> genres=sorted(set.union(genre_iter))				# set.union的对象必须是一个集合
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: descriptor 'union' requires a 'set' object but received a 'generator'
>>> genres=sorted(set.union(*genre_iter))				# 在genre_iter前要有一个*

>>> dummies=DataFrame(np.zeros((len(movies), len(genres))), columns=genres)	# 形成一个len(movies)和len(genres)的dateframe

>>> for i, gen in enumerate(movies.genres):				# 将选定的值改为1
...     dummies.ix[i, gen.split('|')]=1
...
>>> movies_windic=movies.join(dummies.add_prefix('Genre_'))		# 两个dataframe相加
>>> movies_windic.ix[0]
movie_id                                       1
title                           Toy Story (1995)
genres               Animation|Children's|Comedy
Genre_Action                                   0
Genre_Adventure                                0
Genre_Animation                                1
Genre_Children's                               1
Genre_Comedy                                   1

>>> values=np.random.rand(10)
>>> values
array([ 0.61568396,  0.78650502,  0.71044391,  0.64907201,  0.05297702,
        0.84260271,  0.45166837,  0.08015607,  0.24755046,  0.09507061])
>>> bins=[0, 0.4, 0.8, 1]
>>> pd.get_dummies(pd.cut(values, bins))	# 结合get_dummies和诸如cut之类的
   (0, 0.4]  (0.4, 0.8]  (0.8, 1]		# 离散化函数
0       0.0         1.0       0.0
1       0.0         1.0       0.0
2       0.0         1.0       0.0
3       0.0         1.0       0.0
4       1.0         0.0       0.0
5       0.0         0.0       1.0
6       0.0         1.0       0.0
7       1.0         0.0       0.0
8       1.0         0.0       0.0
9       1.0         0.0       0.0

###字符串操作###


#字符串对象方法#
>>> val='a,b, guido'
>>> val.split(',')		# 分隔符是，
['a', 'b', ' guido']

>>> pieces=[x.strip() for x in val.split(',')]		# strip()删除空格
>>> pieces
['a', 'b', 'guido']

>>> first, second, third=pieces
>>> first
'a'
>>> second
'b'
>>> third
'guido'
>>> first, second=pieces		# 必须和pieces中字符串的个数相同
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: too many values to unpack
>>> first+'::'+second+'::'+third
'a::b::guido'
>>> '::'.join(pieces)
'a::b::guido'

>>> 'guido' in val
True
>>> val.index(',')
1
>>> val.find(',')
1
>>> val.index(':')	# 如果找不到，index返回一个异常
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: substring not found
>>> val.find(':')	# find如果找不到，返回-1
-1
>>> val.count(',')	# 字符串中几个，号
2
>>> val.replace(',', '::')	# 取代
'a::b:: guido'
>>> val.replace(',', '::').replace(' ','')
'a::b::guido'

###正则表达式###
>>> regex=re.compile('\s+')	# 编译regex以得到一个可重用的regex对象
>>> regex.split(text)
['foo', 'bar', 'baz', 'qux']

>>> regex.findall(text)		# 得到匹配的所有内容
[' ', '\t ', ' \t']


>>> text="""Dave dave@google.com
... Steve steve@gmail.com
... """
>>> text
'Dave dave@google.com\nSteve steve@gmail.com\n'
>>> pattern=r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'	# 方括号中为匹配的内容，不是正则表达式
>>> regex=re.compile(pattern, flags=re.IGNORECASE)
>>> regex.findall(text)
['dave@google.com', 'steve@gmail.com']

p231




######################
#####绘图和可视化#####
######################

###Figure和subplot###
>>> fig=plt.figure()
>>> ax1=fig.add_subplot(2,2,1)
>>> ax2=fig.add_subplot(2,2,2)
>>> ax3=fig.add_subplot(2,2,3)		
>>> plt.plot(randn(50).cumsum(), 'k--')	# 只有在图3中曲线
[<matplotlib.lines.Line2D object at 0x06299790>]
>>> plt.show()


>>> fig=plt.figure()
>>> ax1=fig.add_subplot(2,2,1)
>>> ax2=fig.add_subplot(2,2,2)
>>> ax3=fig.add_subplot(2,2,3)
>>> _=ax1.hist(randn(100), bins=20, color='k', alpha=0.3)	# 对ax1进行作图 
>>> ax2.scatter(np.arange(30), np.arange(30)+3*randn(30))	# 对ax2进行作图
<matplotlib.collections.PathCollection object at 0x0D56E810>
>>> plt.sca(ax3)						# 选取ax3
>>> plt.plot(randn(50).cumsum(), 'k--')				# 在ax3作图
[<matplotlib.lines.Line2D object at 0x0D95E450>]
>>> plt.show()

###调整subplot周围的间距###
>>> fig, axes=plt.subplots(2, 2, sharex=True, sharey=True)
>>> for i in range(2):
...     for j in range(2):
...             axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)	# 调用不同的axes
>>> plt.subplots_adjust(wspace=0)	# 通过subplots_adjust调整subplot周围的间距
>>> plt.show()

###颜色、标记和线型###
>>> x=range(10)
>>> y=randn(10)
>>> plt.plot(x, y, 'g--')		# 绿色以及虚线
[<matplotlib.lines.Line2D object at 0x0E42E6D0>]
>>> plt.show()

>>> plt.plot(x, y, 'go--')		# 绿实心标记以及虚线
[<matplotlib.lines.Line2D object at 0x0F00B730>]
>>> plt.show()
>>> plt.plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')	# 更为明确的表达方式
[<matplotlib.lines.Line2D object at 0x0F1022F0>]
>>> plt.show()

>>> from numpy.random import randn
>>> data=randn(30).cumsum()
>>> plt.plot(data, 'k--', label='Default')		# 非实际数据点，按线性插值
[<matplotlib.lines.Line2D object at 0x04DAB610>]
>>> plt.legend()
<matplotlib.legend.Legend object at 0x04FA9E50>
>>> plt.show()

>>> plt.plot(data, 'k--', drawstyle='steps-post', label='steps-post')	# 设置插值模式，steps-post 折线图
[<matplotlib.lines.Line2D object at 0x051E8E50>]
>>> plt.legend()
<matplotlib.legend.Legend object at 0x04E13230>
>>> plt.show()


>>> from numpy.random import randn		# 上述两幅图画在一张图里
>>> import matplotlib.pyplot as plt
>>> data=randn(30).cumsum()
>>> plt.plot(data, 'k--', label='Default')
[<matplotlib.lines.Line2D object at 0x055216B0>]
>>> plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
[<matplotlib.lines.Line2D object at 0x05521AB0>]
>>> plt.legend(loc='best')
<matplotlib.legend.Legend object at 0x05521610>
>>> plt.show()




###刻度、标签和图例###

###设置标题、轴标签、刻度以及刻度标签###
>>> fig=plt.figure()
>>> ax=fig.add_subplot(1, 1, 1)
>>> ax.plot(randn(1000).cumsum())
[<matplotlib.lines.Line2D object at 0x05D99DF0>]
>>> ticks=ax.set_xticks([0, 250, 500, 750, 1000])	# 设置坐标刻度
>>> ticks=ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')	# 设置刻度标签
>>> ax.set_title('My first matplotlib plot')
<matplotlib.text.Text object at 0x05D80BD0>
>>> ax.set_xlabel('Stages')
<matplotlib.text.Text object at 0x055035F0>
>>> plt.show()

###添加图例###
>>> fig=plt.figure()
>>> ax=fig.add_subplot(1, 1, 1)
>>> ax.plot(randn(1000).cumsum(), 'k', label='one')
[<matplotlib.lines.Line2D object at 0x0D3ABD10>]
>>> ax.plot(randn(1000).cumsum(), 'k--', label='two')
[<matplotlib.lines.Line2D object at 0x0D3ABE70>]
>>> ax.plot(randn(1000).cumsum(), 'k.', label='three')
[<matplotlib.lines.Line2D object at 0x0D50B490>]
>>> ax.legend(loc='best')		# 生成legend
<matplotlib.legend.Legend object at 0x0D3ABCD0>
>>> plt.show()


####注解以及在subplot上绘图####
PS C:\Users\Weihong> cat spx_fig.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pandas import DataFrame
from pandas import Series
from datetime import datetime

fig=plt.figure()
ax=fig.add_subplot(1, 1, 1)

data=pd.read_csv('spx.csv', index_col=0, parse_dates=True)	# 读取spx.csv 以第0列作为index 解析时间
spx=data['SPX']
#spx_data=web.get_data_yahoo('^GSPC', '1/1/2007', '1/1/2011')	# 从yahoo_finance获得数据
#spx=spx_data['Adj Close']



spx.plot(ax=ax, style='k-')

crisis_data=[(datetime(2007, 10, 11), 'Peak of Bull Market'), (datetime(2008, 3, 12), 'Bear Sterns Fails'), (datetime(2
008, 9, 15), 'Lehman Bankruptcy')]

for date, label in crisis_data:
        ax.annotate(label, xy=(date, spx.asof(date)+70),	# 注释位置的下限
                xytext=(date, spx.asof(date)+200),		# 注释位置的上限
                arrowprops=dict(facecolor='black'),
                horizontalalignment='left', verticalalignment='top')

ax.set_xlim(['1/1/2007', '1/1/2011'])
ax.set_ylim([600, 1800])
ax.set_title('Important dates in 2008-2009 financial crisis')

plt.show()



###图形的绘制###
>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax=fig.add_subplot(1, 1, 1)
>>> rect=plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)	# 矩形 第一点的位置 长度 宽度 颜色 透明度
>>> circ=plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)		# 圆形 圆心的位置 半径 颜色 透明度
>>> pgon=plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)	# 
>>> ax.add_patch(rect)			# ax.add_patch(shp)将图形添加到subplot中
<matplotlib.patches.Rectangle object at 0x04F4FED0>
>>> ax.add_patch(circ)
<matplotlib.patches.Circle object at 0x051A3610>
>>> ax.add_patch(pgon)
<matplotlib.patches.Polygon object at 0x051A37F0>
>>> plt.show()


###将图标保存到文件###
###matplotlib配置###


###pandas中的绘图函数###
###线性图###
>>> s=Series(np.random.randn(100).cumsum(), index=np.arange(0,100,1))
>>> s.plot(use_index=False, xticks=[0, 50, 100])	# 通过use_index=False禁用 将Series对象的索引传给matplotlib并绘制X轴
<matplotlib.axes._subplots.AxesSubplot object at 0x0CEBCC30>	# 通过xticks以及xlim选项进行调节
>>> plt.show()

>>> df=DataFrame(np.random.randn(10, 4).cumsum(0), index=np.arange(0, 100, 10),
... columns=['A', 'B', 'C', 'D'])
>>> df.plot()
<matplotlib.axes._subplots.AxesSubplot object at 0x0DACB970>
>>> plt.show()
>>> df.plot(legend=None)		# 图中没有图例的标签
<matplotlib.axes._subplots.AxesSubplot object at 0x0D358F30>
>>> plt.show()


###柱状图###
>>> fig, axes=plt.subplots(2,1)
>>> data=Series(np.random.rand(5), index=list('abcde'))
>>> data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7)
<matplotlib.axes._subplots.AxesSubplot object at 0x07E5A5B0>
>>> data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)
<matplotlib.axes._subplots.AxesSubplot object at 0x07E8BAD0>
>>> plt.show()	# Series作为index的刻度



>>> df=DataFrame(np.random.rand(6, 4), index=['one', 'two', 'three', 'four', 'five', 'six'],
... columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))
>>> df.plot(kind='bar')		# 会将每一行作为一组，
<matplotlib.axes._subplots.AxesSubplot object at 0x08325A70>
>>> plt.legend(loc='best')	# legend放在没有图的位置
<matplotlib.legend.Legend object at 0x07E732B0>
>>> plt.show()			

>>> df.plot(kind='barh', stacked=True, alpha=0.5)	# stacked=True 每行会被推挤在一起
<matplotlib.axes._subplots.AxesSubplot object at 0x0E3CE090>
>>> plt.legend(loc='best')
<matplotlib.legend.Legend object at 0x0804DF90>
>>> plt.show()


>>> tips=pd.read_csv('tips.csv')
>>> tips.head(10)
   total_bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4
>>> party_counts=pd.crosstab(tips['day'], tips['size'])		# 通过crosstab生成一张交叉表
>>> party_counts						# 表格中每个数字表示个数
size  1   2   3   4  5  6
day
Fri   1  16   1   1  0  0
Sat   2  53  18  13  1  0
Sun   0  39  15  18  3  1
Thur  1  48   4   5  1  3

>>> party_counts=party_counts.ix[:, 2:5]
>>> party_counts
size   2   3   4  5
day
Fri   16   1   1  0
Sat   53  18  13  1
Sun   39  15  18  3
Thur  48   4   5  1
>>> party_counts.sum(1)		# 
day
Fri     18
Sat     85
Sun     75
Thur    58
dtype: int64
>>> party_counts.sum(1).astype(float)
day
Fri     18.0
Sat     85.0
Sun     75.0
Thur    58.0
dtype: float64
>>> party_pcts=party_counts.div(party_counts.sum(1).astype(float), axis=0)	# 先沿着行相加，再按列相除
>>> party_pcts									# axis=1表示按行 , axis=0表示按列
size         2         3         4         5
day
Fri   0.888889  0.055556  0.055556  0.000000
Sat   0.623529  0.211765  0.152941  0.011765
Sun   0.520000  0.200000  0.240000  0.040000
Thur  0.827586  0.068966  0.086207  0.017241
>>> party_pcts.plot(kind='bar', stacked=True)					# 归一化的 堆积柱形图
<matplotlib.axes._subplots.AxesSubplot object at 0x10BF27D0>
>>> plt.show()


###直方图和密度图###
>>> tips.head(10)
   total_bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4
5       25.29  4.71    Male     No  Sun  Dinner     4
6        8.77  2.00    Male     No  Sun  Dinner     2
7       26.88  3.12    Male     No  Sun  Dinner     4
8       15.04  1.96    Male     No  Sun  Dinner     2
9       14.78  3.23    Male     No  Sun  Dinner     2
>>> tips['tip_pct']=tips['tip']/tips['total_bill']
>>> tips.head(5)
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447
1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542
2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587
3       23.68  3.31    Male     No  Sun  Dinner     2  0.139780
4       24.59  3.61  Female     No  Sun  Dinner     4  0.146808
>>> tips['tip_pct'].hist(bins=50)
<matplotlib.axes._subplots.AxesSubplot object at 0x10CABB50>
>>> plt.show()

>>> tips['tip_pct'].plot(kind='kde')		# KDE是标准混合正态分布图
<matplotlib.axes._subplots.AxesSubplot object at 0x11587C70>
>>> plt.show()

###一个x轴 两个y轴###
>>> axe=plt.subplot(111)	# 一个axe对象
>>> axe=tips['tip_pct'].hist(bins=50, grid=None)	# 第一个x-y图
>>> axf=axe.twinx()					# axf是axe公用一个x轴的对象
>>> axf=tips['tip_pct'].plot(kind='kde', color='g', linewidth=2.0, grid=None)	# 生成第二个x-y图
>>> axf.set_ylim((0, 10))
(0, 10)
>>> plt.show()

###散布图###

>>> from pandas import DataFrame
>>> import pandas as pd
>>> import numpy as np
>>> macro=pd.read_csv('macrodata.csv')
>>> macro.head(10)
     year  quarter   realgdp  realcons  realinv  realgovt  realdpi    cpi  \
0  1959.0      1.0  2710.349    1707.4  286.898   470.045   1886.9  28.98
1  1959.0      2.0  2778.801    1733.7  310.859   481.301   1919.7  29.15
2  1959.0      3.0  2775.488    1751.8  289.226   491.260   1916.4  29.35
3  1959.0      4.0  2785.204    1753.7  299.356   484.052   1931.3  29.37
4  1960.0      1.0  2847.699    1770.5  331.722   462.199   1955.5  29.54
5  1960.0      2.0  2834.390    1792.9  298.152   460.400   1966.1  29.55
6  1960.0      3.0  2839.022    1785.8  296.375   474.676   1967.8  29.75
7  1960.0      4.0  2802.616    1788.2  259.764   476.434   1966.6  29.84
8  1961.0      1.0  2819.264    1787.7  266.405   475.854   1984.5  29.81
9  1961.0      2.0  2872.005    1814.3  286.246   480.328   2014.4  29.92

      m1  tbilrate  unemp      pop  infl  realint
0  139.7      2.82    5.8  177.146  0.00     0.00
1  141.7      3.08    5.1  177.830  2.34     0.74
2  140.5      3.82    5.3  178.657  2.74     1.09
3  140.0      4.33    5.6  179.386  0.27     4.06
4  139.6      3.50    5.2  180.007  2.31     1.19
5  140.2      2.68    5.2  180.671  0.14     2.55
6  140.9      2.36    5.6  181.528  2.70    -0.34
7  141.1      2.29    6.3  182.287  1.21     1.08
8  142.1      2.37    6.8  182.992 -0.40     2.77
9  142.9      2.29    7.0  183.691  1.47     0.81
>>> data=macro[['cpi', 'm1', 'tbilrate', 'unemp']]
>>> data.head(5)
     cpi     m1  tbilrate  unemp
0  28.98  139.7      2.82    5.8
1  29.15  141.7      3.08    5.1
2  29.35  140.5      3.82    5.3
3  29.37  140.0      4.33    5.6
4  29.54  139.6      3.50    5.2
>>> trans_data=np.log(data).diff().dropna()
>>> trans_data.head(3)
        cpi        m1  tbilrate     unemp
1  0.005849  0.014215  0.088193 -0.128617
2  0.006838 -0.008505  0.215321  0.038466
3  0.000681 -0.003565  0.125317  0.055060
>>> import matplotlib.pyplot as plt
>>> plt.scatter(trans_data['m1'], trans_data['unemp'])		# 两个量之间的关系
<matplotlib.collections.PathCollection object at 0x05994950>
>>> plt.title('Changes in log %s vs. log %s' % ('m1', 'unemp'))
<matplotlib.text.Text object at 0x05982850>
>>> plt.show()

>>> pd.scatter_matrix(trans_data, diagonal='kde', color='k', alpha=0.3)	# 求得scatter矩阵
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x07290D30>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x0743BF90>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x07488710>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x075414F0>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x07587E70>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x075B79F0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x0768C390>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x076CEA10>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x07700FF0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x077DA690>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x0780F5B0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x0784EC30>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x078E0230>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x079659F0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x07A41090>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x07A72530>]], dtype=object)
>>> plt.show()

###绘制地图：图形化显示海地地震危机数据###
>>> import pandas as pd
>>> from pandas import DataFrame	
>>> data=pd.read_csv('Haiti.csv')	# 读取Haiti.csv
>>> data.head(4)
   Serial                                     INCIDENT TITLE  \
0    4052  * URGENT * Type O blood donations needed in #J...
1    4051                     Food-Aid sent to Fondwa, Haiti
2    4050  how haiti is right now and how it was during t...
3    4049                                        Lost person

      INCIDENT DATE       LOCATION  \
0  05/07/2010 17:26  Jacmel, Haiti
1  28/06/2010 23:06         fondwa
2  24/06/2010 16:21        centrie
3  20/06/2010 21:59         Genoca

                                         DESCRIPTION  \
0  Birthing Clinic in Jacmel #Haiti urgently need...
1  Please help food-aid.org deliver more food to ...
2  i feel so bad for you i know i am supposed to ...
3  We are family members of Juan Antonio Zuniga O...

                                            CATEGORY   LATITUDE   LONGITUDE  \
0        1. Urgences | Emergency, 3. Public Health,   18.233333  -72.533333
1  1. Urgences | Emergency, 2. Urgences logistiqu...  50.226029    5.729886
2  2. Urgences logistiques | Vital Lines, 8. Autr...  22.278381  114.174287
3                          1. Urgences | Emergency,   44.407062    8.933989

  APPROVED VERIFIED
0      YES       NO
1       NO       NO
2       NO       NO
3       NO       NO

>>> data[['INCIDENT DATE', 'LATITUDE', 'LONGITUDE']][:10]	# 列出指定三列
      INCIDENT DATE   LATITUDE   LONGITUDE
0  05/07/2010 17:26  18.233333  -72.533333
1  28/06/2010 23:06  50.226029    5.729886
2  24/06/2010 16:21  22.278381  114.174287
3  20/06/2010 21:59  44.407062    8.933989
4  18/05/2010 16:26  18.571084  -72.334671
5  26/04/2010 13:14  18.593707  -72.310079
6  26/04/2010 14:19  18.482800  -73.638800
7  26/04/2010 14:27  18.415000  -73.195000
8  15/03/2010 10:58  18.517443  -72.236841
9  15/03/2010 11:00  18.547790  -72.410010

>>> data['CATEGORY'][:6]					# CATEGORY列的前六行
0          1. Urgences | Emergency, 3. Public Health,
1    1. Urgences | Emergency, 2. Urgences logistiqu...
2    2. Urgences logistiques | Vital Lines, 8. Autr...
3                            1. Urgences | Emergency,
4                            1. Urgences | Emergency,
5                       5e. Communication lines down,
Name: CATEGORY, dtype: object

>>> data=data[(data.LATITUDE>18)&(data.LATITUDE<20)&(data.LONGITUDE>-75)&(data.LONGITUDE<-70)&data.CATEGORY.notnull()]		# 清楚错误位置信息并移除缺失分类信息


>>> def to_cat_list(catstr):	# 以逗号分开并且去掉空格
...     stripped=(x.strip() for x in catstr.split(','))
...     return [x for x in stripped if x]
...
>>> def get_all_categories(cat_series):		# 将cat_series变成集合集合
...     cat_sets=(set(to_cat_list(x)) for x in cat_series)
...     return sorted(set.union(*cat_sets))
...
>>> def get_english(cat):
...     code, names = cat.split('.')	# 对每一项用逗号分隔
...     if '|' in names:
...             names = names.split('|')[1]	# 如果有'|'分隔符，将|后的值赋给names
...     return code, names.strip()
...

>>> all_cats=get_all_categories(data.CATEGORY)
>>> english_mapping=dict(get_english(x) for x in all_cats)	# 形成字典
>>> english_mapping['2a']
'Food Shortage'
>>> english_mapping['6c']
'Earthquake and aftershocks'

通过添加指标列来分类选取记录
>>> def get_code(seq):
...     return [x.split('.')[0] for x in seq if x]
...
>>> all_codes=get_code(all_cats)	# 得到所有code
>>> code_index=pd.Index(np.unique(all_codes))	# 排除重复值
>>> dummy_frame=DataFrame(np.zeros((len(data), len(code_index))), index=data.index, columns=code_index)		

>>> for row, cat in zip(data.index, data.CATEGORY):	# zip函数
...     codes=get_code(to_cat_list(cat))
...     dummy_frame.ix[row, codes]=1			# 将各项
...
>>> data=data.join(dummy_frame.add_prefix('category_'))		# 合并列


>>> data=pd.read_csv('haiti_data.csv', index_col=0)
>>> data.head(2)


import pandas as pd
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt

data=pd.read_csv('haiti_data.csv', index_col=0)

def basic_haiti_map(ax=None, lllat=17.25, urlat=20.25, lllon=-75, urlon=-71):
    m = Basemap(ax=ax, projection='stere', lon_0=(urlon+lllon)/2, lat_0=(urlat+lllat)/2, llcrnrlat=lllat, urcrnrlat=urlat,
                llcrnrlon=lllon, urcrnrlon=urlon, resolution='f')
    m.drawcoastlines()
    m.drawstates()
    m.drawcountries()
    return m


%matplotlib inline
all_cats=get_all_categories(data.CATEGORY)            
english_mapping=dict(get_english(x) for x in all_cats)

fig, axes=plt.subplots(nrows=2, ncols=2, figsize=(12, 10))
fig.subplots_adjust(hspace=0.05, wspace=0.05)

to_plot=['2a', '1', '3c', '7a']
lllat=17.25; urlat=20.25; lllon=-75; urlon=-71

for code, ax in zip(to_plot, axes.flat):
    m=basic_haiti_map(ax, lllat=lllat, urlat=urlat, lllon=lllon, urlon=urlon)
    cat_data=data[data['category_%s' % code]==1]	# 对于dataframe的布尔运算
    x, y = m(cat_data.LONGITUDE.values, cat_data.LATITUDE.values)
    m.plot(x, y, 'k.', alpha=0.5)
    ax.set_title('%s: %s' % (code, english_mapping[code]))
    
plt.show()	# 形成图像



##################################
#######数据聚合以及分组运算#######
##################################

###GroupBy###
import pandas as pd
from pandas import DataFrame
from pandas import Series
import numpy as np

df=DataFrame({'key1':['a', 'a', 'b', 'b', 'a'], 'key2':['one', 'two', 'one', 'two', 'one'], 'data1':np.random.randn(5), 'data2': np.random.randn(5)})

df
            data1	data2	     key1	key2
0	-1.582099	1.513228	a	one
1	0.528709	1.017830	a	two
2	-0.941078	0.854401	b	one
3	-0.101977	0.249118	b	two
4	-0.541426	1.092519	a	one

grouped=df['data1'].groupby(df['key1'])
grouped		# grouped是一个GroupBy对象，它实际上还没有进行任何计算，只是含有一些有关分组键df['key1']的中间数据
<pandas.core.groupby.SeriesGroupBy object at 0x08634C30>

grouped.mean()
key1
a   -0.531605
b   -0.521527
Name: data1, dtype: float64


means=df['data1'].groupby([df['key1'], df['key2']]).mean()	# 通过两个键对数据进行了分组
means
key1  key2
a     one    -1.061762
      two     0.528709
b     one    -0.941078
      two    -0.101977
Name: data1, dtype: float64

means.unstack()
key2	one	two
key1		
a	-1.061762	0.528709
b	-0.941078	-0.101977

states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])		# 分组键可以是任何长度适当的数组
years=np.array([2005, 2005, 2006, 2005, 2006])

df['data1'].groupby([states, years]).mean()
California  2005    0.528709
            2006   -0.941078
Ohio        2005   -0.842038
            2006   -0.541426
Name: data1, dtype: float64


df.groupby('key1').mean()	# 还可以将列名用作分组键
	   data1	data2	# 结果中没有key2列，这是因为df['key2']不是数值数据，所以被从结果中排除。
key1		
a	-0.531605	1.207859
b	-0.521527	0.551759

>>> df.groupby(['key1', 'key2']).mean()		
              data1     data2
key1 key2
a    one  -0.434662 -0.503116
     two  -0.016132 -0.477130
b    one   0.505572  1.097213
     two  -0.254775 -0.632829

>>> df.groupby(['key1', 'key2']).size()		# 返回一个含有分组大小的Series
key1  key2
a     one     2
      two     1
b     one     1
      two     1
dtype: int64


###对分组进行迭代###
GroupBy对象支持迭代，可以产生一组二元元组（由分组名和数据块组成）
>>> for name, group in df.groupby('key1'):
...     print name
...     print group
...
a
      data1     data2 key1 key2
0 -0.201260  0.238313    a  one
1 -0.016132 -0.477130    a  two
4 -0.668064 -1.244545    a  one
b
      data1     data2 key1 key2
2  0.505572  1.097213    b  one
3 -0.254775 -0.632829    b  two


>>> for (k1, k2), group in df.groupby(['key1', 'key2']):
...     print k1, k2
...     print group
...
a one
      data1     data2 key1 key2
0 -0.201260  0.238313    a  one
4 -0.668064 -1.244545    a  one
a two
      data1    data2 key1 key2
1 -0.016132 -0.47713    a  two
b one
      data1     data2 key1 key2
2  0.505572  1.097213    b  one
b two
      data1     data2 key1 key2
3 -0.254775 -0.632829    b  two

你可以对这些数据片段做任何操作，有一个你可能会觉得有用的
运算，将这些数据片段做成一个字典
>>> pieces=dict(list(df.groupby('key1')))
>>> pieces
{'a':       data1     data2 key1 key2
0 -0.201260  0.238313    a  one
1 -0.016132 -0.477130    a  two
4 -0.668064 -1.244545    a  one, 'b':       data1     data2 key1 key2
2  0.505572  1.097213    b  one
3 -0.254775 -0.632829    b  two}
>>> pieces['b']
      data1     data2 key1 key2
2  0.505572  1.097213    b  one
3 -0.254775 -0.632829    b  two

groupby默认是在axis=0上进行分组的，通过设置
也可以在其他任何轴上进行分组。

>>> df.dtypes
data1    float64
data2    float64
key1      object
key2      object
dtype: object

>>> grouped=df.groupby(df.dtypes, axis=1)	# 根据dtype对列进行分组
>>> grouped
<pandas.core.groupby.DataFrameGroupBy object at 0x056B8C10>
>>> dict(list(grouped))
{dtype('O'):   key1 key2
0    a  one
1    a  two
2    b  one
3    b  two
4    a  one, dtype('float64'):       data1     data2
0 -0.201260  0.238313
1 -0.016132 -0.477130
2  0.505572  1.097213
3 -0.254775 -0.632829
4 -0.668064 -1.244545}


###选取一个或一组列###
>>> df
      data1     data2 key1 key2
0 -0.201260  0.238313    a  one
1 -0.016132 -0.477130    a  two
2  0.505572  1.097213    b  one
3 -0.254775 -0.632829    b  two
4 -0.668064 -1.244545    a  one

>>> df.groupby('key1')['data1']
<pandas.core.groupby.SeriesGroupBy object at 0x056B8CD0>
>>> df.groupby('key1')['data1'].mean()	# 
key1
a   -0.295152
b    0.125398
Name: data1, dtype: float64

>>> df['data1'].groupby(df['key1']).mean()	# 语法糖
key1
a   -0.295152
b    0.125398
Name: data1, dtype: float64



>>> df.groupby('key1')[['data2']].mean()
         data2
key1
a    -0.494454
b     0.232192
>>> df[['data2']].groupby(df['key1']).mean()	# 语法糖
         data2
key1
a    -0.494454
b     0.232192


>>> df.groupby(['key1', 'key2'])[['data2']].mean()	# 只需计算data2列的平均值
              data2
key1 key2
a    one  -0.503116
     two  -0.477130
b    one   1.097213
     two  -0.632829
这种索引操作返回的对象是一个已分组的DataFrame


>>> s_grouped=df.groupby(['key1', 'key2'])['data2']
>>> s_grouped
<pandas.core.groupby.SeriesGroupBy object at 0x056FF930>	# 传入的是标量形式的单个列名
>>> s_grouped.mean()
key1  key2
a     one    -0.503116
      two    -0.477130
b     one     1.097213
      two    -0.632829
Name: data2, dtype: float64


###通过字典或Series进行分组###
>>> people=DataFrame(np.random.randn(5,5), columns=['a', 'b', 'c', 'd', 'e'],
... index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
>>> people.ix[2:3, ['b', 'c']]=np.nan
>>> people
               a         b         c         d         e
Joe     0.783253  0.365270 -0.453690 -0.550623 -0.891470
Steve   0.429961 -0.095747 -0.001165  0.727260 -0.862602
Wes    -0.027483       NaN       NaN -1.226699 -1.094891
Jim     0.503508  0.247347  0.281832  0.139818 -0.293444
Travis -1.143712  0.972726 -0.332614  0.223120  0.770705

>>> mapping={'a':'red', 'b':'red', 'c':'blue', 'd':'blue', 'e':'red', 'f':'orange'}
>>> by_column=people.groupby(mapping, axis=1)
>>> by_column.sum()
            blue       red
Joe    -1.004314  0.257053
Steve   0.726095 -0.528388
Wes    -1.226699 -1.122373
Jim     0.421650  0.457410
Travis -0.109494  0.599719


>>> from pandas import Series
>>> map_series=Series(mapping)
>>> map_series
a       red
b       red
c      blue
d      blue
e       red
f    orange
dtype: object
>>> people.groupby(map_series, axis=1).count()	#Series作为分组键
        blue  red
Joe        2    3
Steve      2    3
Wes        1    2
Jim        2    3
Travis     2    3


###通过函数进行分组###

任何被当做分组键的函数都会在各个索引值上被调用一次，
其返回值就会被用作分组名称。
>>> people.groupby(len).sum()
          a         b         c         d         e
3  1.259278  0.612617 -0.171858 -1.637505 -2.279805
5  0.429961 -0.095747 -0.001165  0.727260 -0.862602
6 -1.143712  0.972726 -0.332614  0.223120  0.770705

>>> key_list = ['one', 'one', 'one', 'two', 'two']
>>> people.groupby([len, key_list]).min()	# [len, key_list]两个分类key
              a         b         c         d         e
3 one -0.027483  0.365270 -0.453690 -1.226699 -1.094891
  two  0.503508  0.247347  0.281832  0.139818 -0.293444
5 one  0.429961 -0.095747 -0.001165  0.727260 -0.862602
6 two -1.143712  0.972726 -0.332614  0.223120  0.770705


###根据索引级别分组###
>>> import pandas as pd
>>> from pandas import DataFrame
>>> import numpy as np
>>> columns=pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],[1,3,5,1,3]], names=['cty', 'tenor'])	# 列名
>>> hier_df = DataFrame(np.random.randn(4, 5), columns=columns)
>>> hier_df
cty          US                            JP
tenor         1         3         5         1         3
0      0.549260 -1.856406 -1.840983  1.653067  0.261075
1     -0.280847  0.405789  3.076363  1.581361 -0.846110
2     -0.757389 -1.178655 -1.091199 -0.760108  1.395836
3      0.537587  0.634596 -0.934766 -2.085189  0.810888
>>> hier_df.groupby(level='cty', axis=1).count()		# 按cty进行分组并进行count操作
cty  JP  US
0     2   3
1     2   3
2     2   3
3     2   3


###数据聚合###
>>> df=DataFrame({'key1':['a', 'a', 'b', 'b', 'a'], 'key2':['one', 'two', 'one', 'two', 'one'], 'data1':np.random.randn(
5), 'data2': np.random.randn(5)})
>>> df
      data1     data2 key1 key2
0  2.344454  1.551601    a  one
1  0.575443 -0.819761    a  two
2 -0.296530  1.565354    b  one
3 -0.569151  1.738871    b  two
4 -0.712274  0.967674    a  one

>>> grouped['data1'].quantile(0.0)
key1
a   -0.712274
b   -0.569151
Name: data1, dtype: float64
>>> grouped['data1'].quantile(0.5)
key1
a    0.575443
b   -0.432840
Name: data1, dtype: float64
>>> grouped['data1'].quantile(0.9)	# a值（2.344+0.7122）*0.9-0.7122
key1
a    1.990652
b   -0.323792
Name: data1, dtype: float64


>>> def peak_to_peak(arr):		# 如果要使用你自己的聚合函数，只需将其传入agg（）
...     return arr.max()-arr.min()
...
>>> grouped.agg(peak_to_peak)		# agg首先对表格进行分组，然后对每组进行函数运算
         data1     data2
key1
a     3.056728  2.371361
b     0.272621  0.173516

>>> grouped.describe()			# describe方法可以用在这里，即使严格来讲，它们并非聚合运算
               data1     data2
key1
a    count  3.000000  3.000000
     mean   0.735875  0.566505
     std    1.534666  1.235533
     min   -0.712274 -0.819761
     25%   -0.068415  0.073957
     50%    0.575443  0.967674
     75%    1.459949  1.259637
     max    2.344454  1.551601
b    count  2.000000  2.000000
     mean  -0.432840  1.652113
     std    0.192772  0.122695
     min   -0.569151  1.565354
     25%   -0.500996  1.608733
     50%   -0.432840  1.652113
     75%   -0.364685  1.695492
     max   -0.296530  1.738871


>>> tips=pd.read_csv('tips.csv')
>>> tips.head(4)
   total_bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2

>>> tips['tip_pct']=tips['tip']/tips['total_bill']	# 增加一列‘tip_pct'
>>> tips[:6]
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447
1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542
2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587
3       23.68  3.31    Male     No  Sun  Dinner     2  0.139780
4       24.59  3.61  Female     No  Sun  Dinner     4  0.146808
5       25.29  4.71    Male     No  Sun  Dinner     4  0.186240

###面向列的多函数应用###
>>> grouped=tips.groupby(['sex', 'smoker'])		# 在df中按'sex'以及'smoker'进行分组
>>> grouped_pct=grouped['tip_pct']			# 选取'tip_pct'列生成SeriesGroupby对象
>>> grouped_pct.agg('mean')				# 采用聚合函数mean() 调用函数要''
sex     smoker
Female  No        0.156921
        Yes       0.182150
Male    No        0.160669
        Yes       0.152771
Name: tip_pct, dtype: float64


>>> grouped_pct.agg(['mean', 'std', peak_to_peak])	# 调用函数要''，但自定义函数不需要''
                   mean       std  peak_to_peak
sex    smoker
Female No      0.156921  0.036421      0.195876
       Yes     0.182150  0.071595      0.360233
Male   No      0.160669  0.041849      0.220186
       Yes     0.152771  0.090588      0.674707
>>> grouped_pct.agg(['mean', 'std', 'peak_to_peak'])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "D:\python\pandas\core\groupby.py", line 2574, in aggregate
    (_level or 0) + 1)
  File "D:\python\pandas\core\groupby.py", line 2636, in _aggregate_multiple_funcs
    results[name] = obj.aggregate(func)
  File "D:\python\pandas\core\groupby.py", line 2570, in aggregate
    return getattr(self, func_or_funcs)(*args, **kwargs)
  File "D:\python\pandas\core\groupby.py", line 498, in __getattr__
    (type(self).__name__, attr))
AttributeError: 'SeriesGroupBy' object has no attribute 'peak_to_peak'
>>> grouped_pct.agg(['mean', std, peak_to_peak])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'std' is not defined


>>> grouped_pct.agg([('foo','mean'),('bar', np.std)])	# 如果传入的是一个由（name，function）元组组成的列表
                    foo       bar			# 则各元组的第一个元素就会被用作DataFrame的列名
sex    smoker						# 可以将这种二元元组列表看做一个有序映射
Female No      0.156921  0.036421
       Yes     0.182150  0.071595
Male   No      0.160669  0.041849
       Yes     0.152771  0.090588

>>> grouped_pct.agg([('foo','mean'), ('bar', np.std)])
                    foo       bar
sex    smoker
Female No      0.156921  0.036421
       Yes     0.182150  0.071595
Male   No      0.160669  0.041849
       Yes     0.152771  0.090588
>>> grouped_pct.agg([('foo','mean'), ('bar', 'std')])		# groupby优化过的函数要''
                    foo       bar				# 其他函数不需要
sex    smoker
Female No      0.156921  0.036421
       Yes     0.182150  0.071595
Male   No      0.160669  0.041849
       Yes     0.152771  0.090588

对于DataFrame，还可以定义一组应用于全部列的函数，或不同的列应用不同的函数。
假设我们想要对tip_pct和total_bill列计算三个统计信息
>>> functions=['count', 'mean', 'max']
>>> result=grouped['tip_pct', 'total_bill'].agg(functions)	# 结果DataFrame拥有层次化的列
>>> result							# 这相当于分别对列进行聚合，然后用concat将结果组装一起
              tip_pct                     total_bill		# 列名用作keys参数
                count      mean       max      count       mean    max
sex    smoker
Female No          54  0.156921  0.252672         54  18.105185  35.83
       Yes         33  0.182150  0.416667         33  17.977879  44.30
Male   No          97  0.160669  0.291990         97  19.791237  48.33
       Yes         60  0.152771  0.710345         60  22.284500  50.81

>>> result['tip_pct']
               count      mean       max
sex    smoker
Female No         54  0.156921  0.252672
       Yes        33  0.182150  0.416667
Male   No         97  0.160669  0.291990
       Yes        60  0.152771  0.710345

>>> ftuples=[('Durchschnitt', 'mean'), ('Abweichung', np.var)]	# 也可以传入带有自定义名称的元组列表
>>> grouped['tip_pct', 'total_bill'].agg(ftuples)
                   tip_pct              total_bill
              Durchschnitt Abweichung Durchschnitt Abweichung
sex    smoker
Female No         0.156921   0.001327    18.105185  53.092422
       Yes        0.182150   0.005126    17.977879  84.451517
Male   No         0.160669   0.001751    19.791237  76.152961
       Yes        0.152771   0.008206    22.284500  98.244673


>>> grouped.agg({'tip':np.max, 'size':'sum'})		# 相对不同的列应用不同的函数
                tip  size				# 向agg传入一个从列名映射到函数的字典
sex    smoker
Female No       5.2   140
       Yes      6.5    74
Male   No       9.0   263
       Yes     10.0   150

>>> grouped.agg({'tip_pct':['min', 'max', 'mean', 'std'], 'size':'sum'})	# 只有将多个函数应用到至少一列时，
                tip_pct                               size			# DataFrame才会拥有层次化的列
                    min       max      mean       std  sum
sex    smoker
Female No      0.056797  0.252672  0.156921  0.036421  140
       Yes     0.056433  0.416667  0.182150  0.071595   74
Male   No      0.071804  0.291990  0.160669  0.041849  263
       Yes     0.035638  0.710345  0.152771  0.090588  150

###以“无索引”的形式返回聚合数据###

>>> tips.head(3)
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447
1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542
2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587

>>> tips.groupby(['sex', 'smoker']).mean()
               total_bill       tip      size   tip_pct
sex    smoker
Female No       18.105185  2.773519  2.592593  0.156921
       Yes      17.977879  2.931515  2.242424  0.182150
Male   No       19.791237  3.113402  2.711340  0.160669
       Yes      22.284500  3.051167  2.500000  0.152771

>>> tips.groupby(['sex', 'smoker'], as_index=False).mean()	# 所有示例中的聚合数据都有由唯一的分组键组成的索引
      sex smoker  total_bill       tip      size   tip_pct	# 可以通过as_index=False以禁用该功能
0  Female     No   18.105185  2.773519  2.592593  0.156921
1  Female    Yes   17.977879  2.931515  2.242424  0.182150
2    Male     No   19.791237  3.113402  2.711340  0.160669
3    Male    Yes   22.284500  3.051167  2.500000  0.152771


###分组级运算和转换###

假设我们想要为一个DataFrame添加一个用于存放各索引分组平均值的列
先聚合再合并
>>> df=DataFrame({'key1':['a', 'a', 'b', 'b', 'a'], 'key2':['one', 'two', 'one', 'two', 'one'], 'data1':np.random.randn(

... 5), 'data2': np.random.randn(5)})
>>> df
      data1     data2 key1 key2
0 -1.721435 -0.278956    a  one
1 -0.767396  0.847943    a  two
2 -0.421089  0.219758    b  one
3 -1.542940 -1.578820    b  two
4 -1.262936 -0.341392    a  one
>>> k1_means=df.groupby('key1').mean().add_prefix('mean_')
>>> k1_means
      mean_data1  mean_data2
key1
a      -1.250589    0.075865
b      -0.982014   -0.679531
>>> pd.merge(df, k1_means, left_on='key1', right_index=True)
      data1     data2 key1 key2  mean_data1  mean_data2
0 -1.721435 -0.278956    a  one   -1.250589    0.075865
1 -0.767396  0.847943    a  two   -1.250589    0.075865
4 -1.262936 -0.341392    a  one   -1.250589    0.075865
2 -0.421089  0.219758    b  one   -0.982014   -0.679531
3 -1.542940 -1.578820    b  two   -0.982014   -0.679531


###transform方法###
>>> people=DataFrame(np.random.randn(5,5), columns=['a', 'b', 'c', 'd', 'e'],
... index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
>>> people
               a         b         c         d         e
Joe    -0.939202  0.846894 -0.272412 -0.941374 -0.318192
Steve   1.174710  1.149915  0.090326 -0.242829  0.567943
Wes    -1.424880 -0.674782 -1.095908  1.321413 -1.169102
Jim     1.625220  0.630454  0.608164 -0.561225  0.404548
Travis -1.270246  1.374656 -0.168025  0.062955  1.421811

>>> key=['one', 'two', 'one', 'two', 'one']
>>> people.groupby(key).mean()
            a         b         c         d         e
one -1.211443  0.515589 -0.512115  0.147664 -0.021828
two  1.399965  0.890184  0.349245 -0.402027  0.486246
>>> people.groupby(key).transform(np.mean)			# transform会将一个函数应用到各个分组
               a         b         c         d         e	# 然后将结果放置到适当的位置上
Joe    -1.211443  0.515589 -0.512115  0.147664 -0.021828	# 如果各分组产生的是一个标量值，则该值就会被广播出去
Steve   1.399965  0.890184  0.349245 -0.402027  0.486246
Wes    -1.211443  0.515589 -0.512115  0.147664 -0.021828
Jim     1.399965  0.890184  0.349245 -0.402027  0.486246
Travis -1.211443  0.515589 -0.512115  0.147664 -0.021828

>>> def demean(arr):
...     return arr-arr.mean()
...
>>> demeaned=people.groupby(key).transform(demean)		# 各组中减去平均值
>>> demeaned
               a         b         c         d         e
Joe     0.272240  0.331305  0.239703 -1.089039 -0.296365
Steve  -0.225255  0.259731 -0.258919  0.159198  0.081697
Wes    -0.213437 -1.190371 -0.583793  1.173749 -1.147274
Jim     0.225255 -0.259731  0.258919 -0.159198 -0.081697
Travis -0.058804  0.859067  0.344090 -0.084710  1.443639


###apply：一般性的“拆分-应用-合并”###
>>> import pandas as pd
>>> import numpy as np
>>> from pandas import DataFrame
>>> from pandas import Series

>>> tips=pd.read_csv('tips.csv')
>>> tips.head(4)
   total_bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
>>> tips['tip_pct']=tips['tip']/tips['total_bill']
>>> tips[:6]
   total_bill   tip     sex smoker  day    time  size   tip_pct
0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447
1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542
2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587
3       23.68  3.31    Male     No  Sun  Dinner     2  0.139780
4       24.59  3.61  Female     No  Sun  Dinner     4  0.146808
5       25.29  4.71    Male     No  Sun  Dinner     4  0.186240

>>> def top(df, n=5, column='tip_pct'):
...     return df.sort_index(by=column)[-n:]
...


>>> top(tips, n=6)
     total_bill   tip     sex smoker  day    time  size   tip_pct
109       14.31  4.00  Female    Yes  Sat  Dinner     2  0.279525
183       23.17  6.50    Male    Yes  Sun  Dinner     4  0.280535
232       11.61  3.39    Male     No  Sat  Dinner     2  0.291990
67         3.07  1.00  Female    Yes  Sat  Dinner     1  0.325733
178        9.60  4.00  Female    Yes  Sun  Dinner     2  0.416667
172        7.25  5.15    Male    Yes  Sun  Dinner     2  0.710345

>>> tips.groupby('smoker').apply(top)
            total_bill   tip     sex smoker   day    time  size   tip_pct
smoker
No     88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746
       185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663
       51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672
       149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312
       232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990
Yes    109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525
       183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535
       67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733
       178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667
       172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345

top函数在DataFrame的各个片段上调用，然后结果由pandas.concat组装到一起，
并以分组名称进行了标记。于是，最终结果就有了一个层次化索引，其内层索引值来自
原DataFrame。

>>> tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')	# 对每一分组通过apply运用top（）函数
                 total_bill    tip     sex smoker   day    time  size  \
smoker day
No     Fri  94        22.75   3.25  Female     No   Fri  Dinner     2
       Sat  212       48.33   9.00    Male     No   Sat  Dinner     4
       Sun  156       48.17   5.00    Male     No   Sun  Dinner     6
       Thur 142       41.19   5.00    Male     No  Thur   Lunch     5
Yes    Fri  95        40.17   4.73    Male    Yes   Fri  Dinner     4
       Sat  170       50.81  10.00    Male    Yes   Sat  Dinner     3
       Sun  182       45.35   3.50    Male    Yes   Sun  Dinner     3
       Thur 197       43.11   5.00  Female    Yes  Thur   Lunch     4

                  tip_pct
smoker day
No     Fri  94   0.142857
       Sat  212  0.186220
       Sun  156  0.103799
       Thur 142  0.121389
Yes    Fri  95   0.117750
       Sat  170  0.196812
       Sun  182  0.077178
       Thur 197  0.115982


>>> result=tips.groupby('smoker')['tip_pct'].describe()		# 对每个分组运用describe函数
>>> result
smoker
No      count    151.000000
        mean       0.159328
        std        0.039910
        min        0.056797
        25%        0.136906
        50%        0.155625
        75%        0.185014
        max        0.291990
Yes     count     93.000000
        mean       0.163196
        std        0.085119
        min        0.035638
        25%        0.106771
        50%        0.153846
        75%        0.195059
        max        0.710345
Name: tip_pct, dtype: float64
f=lambda x:x.describe()
grouped.apply(f)

>>> result.unstack('smoker')		
smoker          No        Yes
count   151.000000  93.000000
mean      0.159328   0.163196
std       0.039910   0.085119
min       0.056797   0.035638
25%       0.136906   0.106771
50%       0.155625   0.153846
75%       0.185014   0.195059
max       0.291990   0.710345

禁止分组键
>>> tips.groupby('smoker', group_keys=False).apply(top, n=2)
     total_bill   tip     sex smoker   day    time  size   tip_pct
149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312
232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990
178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667
172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345
>>> tips.groupby('smoker').apply(top, n=2)
            total_bill   tip     sex smoker   day    time  size   tip_pct
smoker
No     149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312
       232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990
Yes    178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667
       172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345

###分位数和桶分析###
>>> frame=DataFrame({'data1':np.random.randn(1000),'data2':np.random.randn(1000)})
>>> factor=pd.cut(frame.data1, 4)
>>> factor[:10]
0    (-1.994, -0.231]
1     (-0.231, 1.533]
2    (-3.765, -1.994]
3    (-1.994, -0.231]
4     (-0.231, 1.533]
5      (1.533, 3.296]
6      (1.533, 3.296]
7    (-1.994, -0.231]
8     (-0.231, 1.533]
9     (-0.231, 1.533]
Name: data1, dtype: category
Categories (4, object): [(-3.765, -1.994] < (-1.994, -0.231] < (-0.231, 1.533] < (1.533, 3.296]]
>>> frame[:10]
      data1     data2
0 -0.589495 -0.180395
1  0.109874 -0.305354
2 -2.192129  0.314682
3 -1.639531 -0.113811
4  1.374131  1.258100
5  1.665788 -0.137451
6  1.549833  1.563430
7 -0.895533  0.334040
8  0.503088  1.362532
9  0.308722  1.330346

>>> def get_stats(group):
...     return {'min':group.min(), 'max':group.max(),'count':group.count(),'mean':group.mean()}
...
>>> grouped = frame.data2.groupby(factor)	# 由cut返回的Factor对象可直接用于groupby
>>> grouped.apply(get_stats).unstack()
                  count       max      mean       min
data1
(-3.765, -1.994]   28.0  2.198493  0.193092 -1.468494
(-1.994, -0.231]  393.0  3.258157 -0.004869 -2.864043
(-0.231, 1.533]   526.0  2.872361  0.032062 -3.270485
(1.533, 3.296]     53.0  2.159796 -0.092150 -2.786031


>>> grouping=pd.qcut(frame.data1, 10)
>>> grouping.head(3)	# 直接显示属于第几个分组
0    (-0.885, -0.571]
1    (-0.00218, 0.25]
2    [-3.758, -1.312]
Name: data1, dtype: category
Categories (10, object): [[-3.758, -1.312] < (-1.312, -0.885] < (-0.885, -0.571] < (-0.571, -0.3] < ... <
                          (0.25, 0.525] < (0.525, 0.778] < (0.778, 1.21] < (1.21, 3.296]]
>>> grouping=pd.qcut(frame.data1, 10, labels=False)	
>>> grouping.head(3)	# 属于第几个分组
0    2
1    5
2    0
Name: data1, dtype: int32

>>> grouping=pd.qcut(frame.data1, 10, labels=False)	# 只获取分位数的编号
>>> grouped=frame.data2.groupby(grouping)
>>> grouped.apply(get_stats).unstack()
       count       max      mean       min
data1
0      100.0  3.258157  0.096630 -2.864043
1      100.0  2.605247  0.007822 -2.486652
2      100.0  2.888013 -0.019419 -2.184034
3      100.0  2.441048 -0.027338 -2.139440
4      100.0  2.206019 -0.075432 -3.270485
5      100.0  2.321558 -0.104430 -2.223344
6      100.0  2.872361  0.058656 -3.040754
7      100.0  2.108808  0.126443 -1.899453
8      100.0  2.645757  0.100068 -2.547324
9      100.0  2.175372 -0.008262 -2.786031
>>> frame.data2.max()
3.2581566603391283
>>> frame.data1.max()
3.2960757936808283


###用特定于分组的值填充缺失值###
>>> s=[1,2,3,4,5,6,7,8,9]
>>> s[::2]		# 间隔为2
[1, 3, 5, 7, 9]
>>> s[::-1]		# 倒叙
[9, 8, 7, 6, 5, 4, 3, 2, 1]

>>> s = Series(np.random.randn(6))
>>> s[::2]=np.nan
>>> s
0         NaN
1    0.225085
2         NaN
3   -2.469086
4         NaN
5   -0.532430
dtype: float64
>>> s.fillna(s.mean())		# 用fillna给空缺值填入s的平均值
0   -0.925477
1    0.225085
2   -0.925477
3   -2.469086
4   -0.925477
5   -0.532430
dtype: float64


你需要对不同的分组填充不同的值，只需将数据分组，
并使用apply和一个能够对各数据块调用fillna的函数
>>> states=['Ohio', 'New York', 'Vermont', 'Florida', 'Oregon', 'Nevada', 'California', 'Idaho']
>>> group_key=['East']*4+['West']*4
>>> data = Series(np.random.randn(8), index=states)
>>> data[['Vermont', 'Nevada', 'Idaho']]=np.nan
>>> data
Ohio         -1.221589
New York      1.078202
Vermont            NaN
Florida      -0.292261
Oregon        0.419586
Nevada             NaN
California    0.577395
Idaho              NaN
dtype: float64
>>> data.groupby(group_key).mean()
East   -0.145216
West    0.498491
dtype: float64
>>> fill_mean = lambda g: g.fillna(g.mean())
>>> data.groupby(group_key).apply(fill_mean)	# 用分组平均值来填充nan值， apply对每一数据块运用fill_mean函数
Ohio         -1.221589
New York      1.078202
Vermont      -0.145216
Florida      -0.292261
Oregon        0.419586
Nevada        0.498491
California    0.577395
Idaho         0.498491
dtype: float64

也可以在代码中预定义各组的填充值。由于分组具有一个name属性，
>>> fill_values={'East':0.5, 'West':-1}
>>> fill_func=lambda g:g.fillna(fill_values[g.name])
>>> data.groupby(group_key).apply(fill_func)	# apply是将函数运用于每个模块，因此fill_func中的g
Ohio         -1.221589				# 是模块的含义
New York      1.078202
Vermont       0.500000
Florida      -0.292261
Oregon        0.419586
Nevada       -1.000000
California    0.577395
Idaho        -1.000000
dtype: float64


示例：随机采样和排列
python list中append()与extend()用法分享
1. 列表可包含任何数据类型的元素，单个列表中的元素无须全为同一类型。 
2.  append() 方法向列表的尾部添加一个新的元素。只接受一个参数。
3.  extend()方法只接受一个列表或字符串作为参数，并将该参数的每个元素都添加到原有的列表中。
>>> mylist=[1,2,0, 'abc']
>>> mylist
[1, 2, 0, 'abc']
>>> mylist.append(4)
>>> mylist
[1, 2, 0, 'abc', 4]
>>> mylist.append(['re', 'haha'])
>>> mylist
[1, 2, 0, 'abc', 4, ['re', 'haha']]
>>> mylist.extend(['a', 12])
>>> mylist
[1, 2, 0, 'abc', 4, ['re', 'haha'], 'a', 12]
>>> mylist.extend('1')
>>> mylist
[1, 2, 0, 'abc', 4, ['re', 'haha'], 'a', 12, '1']


>>> suits=['H', 'S', 'C', 'D']
>>> card_val=(range(1,11)+[10]*3)*4
>>> base_names=['A']+range(2,11)+['J', 'K', 'Q']
>>> cards=[]
>>> for suit in ['H', 'S', 'C', 'D']:
...     cards.extend(str(num) + suit for num in base_names)
...
>>> deck = Series(card_val, index=cards)
>>> deck[:13]
AH      1
2H      2
3H      3
4H      4
5H      5
6H      6
7H      7
8H      8
9H      9
10H    10
JH     10
KH     10
QH     10
dtype: int64

>>> deck.take([1,3])		# Series的take函数，取Series中第1个和第3个元素（从0开始）
2H    2
4H    4
dtype: int64
>>> deck.take(np.random.permutation(len(deck))[:3])	# np.random.permutation()返回一个随机顺序并取前三
8D     8
5D     5
KS    10
dtype: int64


>>> def draw(deck, n=5):			# 随机抽取5张牌
...     return deck.take(np.random.permutation(len(deck))[:n])
...
>>> draw(deck)
QH    10
AH     1
2H     2
6D     6
6C     6
dtype: int64

假设你想要从每种花色中随机抽取两张牌。由于花色是牌名最后一个字符
所以我们可以据此进行分组，并使用apply
>>> get_suit = lambda c:c[-1]
>>> deck.groupby(get_suit).apply(draw, n=2)	# groupby函数作用于deck这个Series中的index
C  4C     4					# 并以此进行分组
   KC    10
D  7D     7
   4D     4
H  5H     5
   AH     1
S  7S     7
   4S     4
dtype: int64

>>> deck.groupby(get_suit, group_keys=False).apply(draw, n=2)
AC      1
QC     10
AD      1
6D      6
10H    10
8H      8
6S      6
JS     10
dtype: int64

###示例：分组加权平均数和相关系数###
>>> df=DataFrame({'category':['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], 'data':np.random.randn(8), 'weights':np.random.ra
nd(8)})
>>> df
  category      data   weights
0        a -1.792611  0.336671
1        a -0.289157  0.816635
2        a  0.038686  0.674403
3        a  1.531856  0.516637
4        b  1.700924  0.995365
5        b  1.945015  0.666757
6        b  2.535083  0.979162
7        b  1.675543  0.137943
>>> grouped=df.groupby('category')
>>> get_wavg=lambda g:np.average(g['data'], weights=g['weights'])	# 对于每个模块进行加权平均
>>> grouped.apply(get_wavg)
category
a   -0.009449
b    2.052110
dtype: float64


>>> close_px = pd.read_csv('stock_px.csv', parse_dates=True, index_col=0)	# 读取csv文件，解析日期，并且不用index，以日期为index
>>> close_px[-4:]								# close_px后四行
              AAPL   MSFT    XOM      SPX
2011-10-11  400.29  27.00  76.27  1195.54
2011-10-12  402.19  26.96  77.16  1207.25
2011-10-13  408.43  27.18  76.37  1203.66
2011-10-14  422.00  27.27  78.11  1224.58
>>> rets=close_px.pct_change().dropna()						# 运用pct_change()计算百分比并且dropna去除NaN值
>>> rets.head(3)
                AAPL      MSFT       XOM       SPX
2003-01-03  0.006757  0.001421  0.000684 -0.000484
2003-01-06  0.000000  0.017975  0.024624  0.022474
2003-01-07 -0.002685  0.019052 -0.033712 -0.006545
>>> spx_corr = lambda x:x.corrwith(x['SPX'])					# 定义spx_corr函数 x是一个分组后的模块，x中的每列与x['SPX']求相关系数
>>> by_year=rets.groupby(lambda x: x.year)					# groupby只会对index列进行解析并且分组
>>> by_year.apply(spx_corr)							# 对每组运用spx_corr函数
          AAPL      MSFT       XOM  SPX
2003  0.541124  0.745174  0.661265  1.0
2004  0.374283  0.588531  0.557742  1.0
2005  0.467540  0.562374  0.631010  1.0
2006  0.428267  0.406126  0.518514  1.0
2007  0.508118  0.658770  0.786264  1.0
2008  0.681434  0.804626  0.828303  1.0
2009  0.707103  0.654902  0.797921  1.0
2010  0.710105  0.730118  0.839057  1.0
2011  0.691931  0.800996  0.859975  1.0

>>> by_year.apply(lambda g:g['AAPL'].corr(g['MSFT']))				# 计算列与列之间的相关系数 注意这里是corr（）函数
2003    0.480868								# 前面是corrwith（）函数
2004    0.259024
2005    0.300093
2006    0.161735
2007    0.417738
2008    0.611901
2009    0.432738
2010    0.571946
2011    0.581987
dtype: float64

###面向分组的线性回归###
>>> import pandas as pd
>>> import numpy as np
>>> from pandas import DataFrame
>>> from pandas import Series
>>> close_px=pd.read_csv('stock_px.csv', parse_dates=True, index_col=0)
>>> close_px[-4:]
              AAPL   MSFT    XOM      SPX
2011-10-11  400.29  27.00  76.27  1195.54
2011-10-12  402.19  26.96  77.16  1207.25
2011-10-13  408.43  27.18  76.37  1203.66
2011-10-14  422.00  27.27  78.11  1224.58
>>> rets=close_px.pct_change().dropna()
>>> by_year=rets.groupby(lambda x: x.year)

def regress(data, yvar, xvars):
    Y = data[yvar]
    X = data[xvars]
    X['intercept']=1.0
    result=sm.OLS(Y,X).fit()
    return result.params

by_year.apply(regress, 'AAPL', ['SPX'])

# 透视表和交叉表

































####数据处理####

###从网络获得数据###
>>> import pandas as pd
>>> from pandas import DataFrame
>>> from pandas import Series
>>> data_url = "https://raw.githubusercontent.com/alstat/Analysis-with-Programming/master/2014/Python/Numerical-Descript
ions-of-the-Data/data.csv"
>>> df=pd.read_csv(data_url)
>>> df


>>> print df.head()	# head of data 默认5行
    Abra  Apayao  Benguet  Ifugao  Kalinga
0   1243    2934      148    3300    10553
1   4158    9235     4287    8063    35257
2   1787    1922     1955    1074     4544
3  17152   14501     3536   19607    31687
4   1266    2385     2530    3315     8520
>>> print df.head(4)
    Abra  Apayao  Benguet  Ifugao  Kalinga
0   1243    2934      148    3300    10553
1   4158    9235     4287    8063    35257
2   1787    1922     1955    1074     4544
3  17152   14501     3536   19607    31687

>>> print df.tail(7)	# tail of the data
     Abra  Apayao  Benguet  Ifugao  Kalinga
72   6209    6335     3530   15560    24385
73  13316   38613     2585    7746    66148
74   2505   20878     3519   19737    16513
75  60303   40065     7062   19422    61808
76   6311    6756     3561   15910    23349
77  13345   38902     2583   11096    68663
78   2623   18264     3745   16787    16900


>>> print df.columns	# extract columns name
Index([u'Abra', u'Apayao', u'Benguet', u'Ifugao', u'Kalinga'], dtype='object')
>>> print df.index	# extract index name
RangeIndex(start=0, stop=79, step=1)

###打印第一列的前5行
>>> print df.ix[:, 0].head()
0     1243
1     4158
2     1787
3    17152
4     1266
Name: Abra, dtype: int64


>>> print df.ix[10:20, 0:3]	# 为了取出从11到20行的前3列数据
     Abra  Apayao  Benguet
10    981    1311     2560
11  27366   15093     3039
12   1100    1701     2382
13   7212   11001     1088
14   1048    1427     2847
15  25679   15661     2942
16   1055    2191     2119
17   5437    6461      734
18   1029    1183     2302
19  23710   12222     2598
20   1091    2343     2654


>>> print df.describe()		# 通过describe属性，对数据的统计特性进行描述
               Abra        Apayao      Benguet        Ifugao       Kalinga
count     79.000000     79.000000    79.000000     79.000000     79.000000
mean   12874.379747  16860.645570  3237.392405  12414.620253  30446.417722
std    16746.466945  15448.153794  1588.536429   5034.282019  22245.707692
min      927.000000    401.000000   148.000000   1074.000000   2346.000000
25%     1524.000000   3435.500000  2328.000000   8205.000000   8601.500000
50%     5790.000000  10588.000000  3202.000000  13044.000000  24494.000000
75%    13330.500000  33289.000000  3918.500000  16099.500000  52510.500000
max    60303.000000  54625.000000  8813.000000  21031.000000  68663.000000

>>> from scipy import stats as ss
>>> print ss.ttest_1samp(a=df.ix[:, 'Abra'], popmean=15000)
Ttest_1sampResult(statistic=-1.1281738488299586, pvalue=0.26270472069109496)
看到p值是0.267远大于α等于0.05 拒绝H0









###数据可视化###

水平bar图
PS C:\Users\Weihong> cat pyplot_1.py
"""
Simple demo of a horizontal bar chart.
"""

import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np

# Example data
#cols = ['blue', 'green', 'red', 'cyan', 'yellow']	# 可以通过颜色列表的方式给每个bar添加颜色
people=('Tom', 'Dick','Harry', 'Slim', 'Jim')
y_pos=np.arange(len(people))	
#y_pos=np.arange(1,6)
performance=3+10*np.random.rand(len(people))
color=cm.jet(performance/max(performance))		# 也可以通过colormap来给bar添加颜色
error=np.random.rand(len(people))

#plt.barh(y_pos, performance, xerr=error, align='center', alpha=0.4)
plt.barh(y_pos, performance, xerr=error, linewidth=0, color=color, ecolor='red', align='center', alpha=0.4)	# xerr表示误差 alpha表示透明度
plt.yticks(y_pos, people)				# y轴上的命名
plt.xlabel('Performance')
plt.title('How fast do you want to go today?')

plt.show()









